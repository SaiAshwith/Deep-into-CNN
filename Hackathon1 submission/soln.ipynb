{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv(\"train.csv\")\n",
    "datt = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_65</th>\n",
       "      <th>feature_66</th>\n",
       "      <th>feature_67</th>\n",
       "      <th>feature_68</th>\n",
       "      <th>feature_69</th>\n",
       "      <th>feature_70</th>\n",
       "      <th>feature_71</th>\n",
       "      <th>feature_72</th>\n",
       "      <th>feature_73</th>\n",
       "      <th>feature_74</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.00000</td>\n",
       "      <td>200000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>99999.500000</td>\n",
       "      <td>0.972710</td>\n",
       "      <td>1.168365</td>\n",
       "      <td>2.219325</td>\n",
       "      <td>2.296735</td>\n",
       "      <td>0.793530</td>\n",
       "      <td>1.431105</td>\n",
       "      <td>1.010695</td>\n",
       "      <td>0.673090</td>\n",
       "      <td>1.94398</td>\n",
       "      <td>...</td>\n",
       "      <td>1.798040</td>\n",
       "      <td>0.508695</td>\n",
       "      <td>1.827300</td>\n",
       "      <td>0.910370</td>\n",
       "      <td>1.603585</td>\n",
       "      <td>1.219210</td>\n",
       "      <td>0.806895</td>\n",
       "      <td>1.282925</td>\n",
       "      <td>2.94021</td>\n",
       "      <td>0.632005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>57735.171256</td>\n",
       "      <td>3.941836</td>\n",
       "      <td>3.993407</td>\n",
       "      <td>6.476570</td>\n",
       "      <td>7.551858</td>\n",
       "      <td>2.935785</td>\n",
       "      <td>5.162746</td>\n",
       "      <td>3.949231</td>\n",
       "      <td>2.234949</td>\n",
       "      <td>3.93133</td>\n",
       "      <td>...</td>\n",
       "      <td>5.053014</td>\n",
       "      <td>1.867330</td>\n",
       "      <td>7.188924</td>\n",
       "      <td>3.835182</td>\n",
       "      <td>4.877679</td>\n",
       "      <td>4.826003</td>\n",
       "      <td>2.458741</td>\n",
       "      <td>4.261420</td>\n",
       "      <td>10.78465</td>\n",
       "      <td>3.925310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>49999.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>99999.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>149999.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>199999.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>38.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>130.00000</td>\n",
       "      <td>52.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id      feature_0      feature_1      feature_2  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean    99999.500000       0.972710       1.168365       2.219325   \n",
       "std     57735.171256       3.941836       3.993407       6.476570   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%     49999.750000       0.000000       0.000000       0.000000   \n",
       "50%     99999.500000       0.000000       0.000000       0.000000   \n",
       "75%    149999.250000       1.000000       1.000000       1.000000   \n",
       "max    199999.000000      61.000000      51.000000      64.000000   \n",
       "\n",
       "           feature_3      feature_4      feature_5      feature_6  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        2.296735       0.793530       1.431105       1.010695   \n",
       "std         7.551858       2.935785       5.162746       3.949231   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         1.000000       0.000000       1.000000       0.000000   \n",
       "max        70.000000      38.000000      76.000000      43.000000   \n",
       "\n",
       "           feature_7     feature_8  ...     feature_65     feature_66  \\\n",
       "count  200000.000000  200000.00000  ...  200000.000000  200000.000000   \n",
       "mean        0.673090       1.94398  ...       1.798040       0.508695   \n",
       "std         2.234949       3.93133  ...       5.053014       1.867330   \n",
       "min         0.000000       0.00000  ...       0.000000       0.000000   \n",
       "25%         0.000000       0.00000  ...       0.000000       0.000000   \n",
       "50%         0.000000       0.00000  ...       0.000000       0.000000   \n",
       "75%         0.000000       2.00000  ...       1.000000       0.000000   \n",
       "max        30.000000      38.00000  ...      54.000000      24.000000   \n",
       "\n",
       "          feature_67     feature_68     feature_69     feature_70  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        1.827300       0.910370       1.603585       1.219210   \n",
       "std         7.188924       3.835182       4.877679       4.826003   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         1.000000       1.000000       2.000000       1.000000   \n",
       "max        79.000000      55.000000      65.000000      67.000000   \n",
       "\n",
       "          feature_71     feature_72    feature_73     feature_74  \n",
       "count  200000.000000  200000.000000  200000.00000  200000.000000  \n",
       "mean        0.806895       1.282925       2.94021       0.632005  \n",
       "std         2.458741       4.261420      10.78465       3.925310  \n",
       "min         0.000000       0.000000       0.00000       0.000000  \n",
       "25%         0.000000       0.000000       0.00000       0.000000  \n",
       "50%         0.000000       0.000000       0.00000       0.000000  \n",
       "75%         1.000000       1.000000       1.00000       0.000000  \n",
       "max        30.000000      61.000000     130.00000      52.000000  \n",
       "\n",
       "[8 rows x 76 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1 = dat.copy()\n",
    "datt1 = datt.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['id', 'feature_0'], dtype='object'),\n",
       " Index(['id', 'feature_0'], dtype='object'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat1.columns[:2], datt1.columns[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Class_6\n",
       "1    Class_6\n",
       "2    Class_2\n",
       "Name: target, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat1.target[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class_6    51811\n",
       "Class_8    51763\n",
       "Class_9    25542\n",
       "Class_2    24431\n",
       "Class_3    14798\n",
       "Class_7    14769\n",
       "Class_1     9118\n",
       "Class_4     4704\n",
       "Class_5     3064\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat1.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 30, 31, 35, 36, 37, 38,\n",
      "            39, 41, 51],\n",
      "           dtype='int64')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dat1.feature_1.value_counts().keys().sort_values()) \n",
    "dat1.feature_1.value_counts().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"Class_{}\"\n",
    "lisp = [word.format(i) for i in range(1,10)]\n",
    "lisf = [i for i in range(1,10)]\n",
    "dat1.target.replace(lisp,lisf,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    51811\n",
       "8    51763\n",
       "9    25542\n",
       "2    24431\n",
       "3    14798\n",
       "7    14769\n",
       "1     9118\n",
       "4     4704\n",
       "5     3064\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat1.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "iden = datt1.id.copy()\n",
    "dat1.drop(axis = 1, labels = 'id', inplace = True)\n",
    "datt1.drop(axis = 1, labels = 'id', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       " 0          0          0          6          1          0          0   \n",
       " 1          0          0          0          0          0          0   \n",
       " 2          0          0          0          0          0          1   \n",
       " 3          0          0          7          0          1          5   \n",
       " 4          1          0          0          0          0          0   \n",
       " \n",
       "    feature_6  feature_7  feature_8  feature_9  ...  feature_66  feature_67  \\\n",
       " 0          0          0          7          0  ...           0           0   \n",
       " 1          0          0          0          0  ...           2           0   \n",
       " 2          0          3          0          0  ...           0           0   \n",
       " 3          2          2          0          1  ...           0           4   \n",
       " 4          0          0          0          0  ...           0           0   \n",
       " \n",
       "    feature_68  feature_69  feature_70  feature_71  feature_72  feature_73  \\\n",
       " 0           0           0           0           0           2           0   \n",
       " 1           0           0           0           0           0           1   \n",
       " 2           0           0           1           0           0           0   \n",
       " 3           0           2           2           0           4           3   \n",
       " 4           0           0           0           0           0           0   \n",
       " \n",
       "    feature_74  target  \n",
       " 0           0       6  \n",
       " 1           0       6  \n",
       " 2           0       2  \n",
       " 3           0       8  \n",
       " 4           0       2  \n",
       " \n",
       " [5 rows x 76 columns],\n",
       "    feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       " 0          0          0          0          0          0          0   \n",
       " 1          1          2          0          0          0          0   \n",
       " 2          0          1          7          1          0          0   \n",
       " 3          0          0          0          4          3          1   \n",
       " 4          0          0          5          0          0          0   \n",
       " \n",
       "    feature_6  feature_7  feature_8  feature_9  ...  feature_65  feature_66  \\\n",
       " 0          0          0          0          0  ...           0           0   \n",
       " 1          0          0          0          0  ...           3           1   \n",
       " 2          0          0          6          0  ...           3           0   \n",
       " 3          0          0          0          0  ...           0           0   \n",
       " 4          0          0          0          8  ...           0           0   \n",
       " \n",
       "    feature_67  feature_68  feature_69  feature_70  feature_71  feature_72  \\\n",
       " 0           0           0           0           0           0           0   \n",
       " 1           3           0           0           0           0           3   \n",
       " 2           0           0           0           3           0           2   \n",
       " 3           0           1           0           0           0           4   \n",
       " 4           0           0           0           0           0           0   \n",
       " \n",
       "    feature_73  feature_74  \n",
       " 0           0           0  \n",
       " 1           0           0  \n",
       " 2           0           0  \n",
       " 3           0           0  \n",
       " 4           1           0  \n",
       " \n",
       " [5 rows x 75 columns])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat1[:].head(), datt1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'target'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = [str(i) for i in range(0,75)]\n",
    "datt1.columns = col\n",
    "col.append('target')\n",
    "dat1.columns = col\n",
    "col.pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['0', '1', '2', '3', '4'], dtype='object'),\n",
       " Index(['0', '1', '2', '3', '4'], dtype='object'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat1.columns[:5], datt1.columns[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dat2 = dat1.copy()\\nfor i in range(0,74):\\n    for j in range(i+1,75):\\n        dat2[f\"{i}{j}\"] = dat2[f\"{i}\"]*dat2[f\"{j}\"]\\n        print(f\"{i}{j}\")\\ndat2.columns'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''dat2 = dat1.copy()\n",
    "for i in range(0,74):\n",
    "    for j in range(i+1,75):\n",
    "        dat2[f\"{i}{j}\"] = dat2[f\"{i}\"]*dat2[f\"{j}\"]\n",
    "        print(f\"{i}{j}\")\n",
    "dat2.columns'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i in range(0,75):\\n    dat2[f\"{i}{i}\"] = dat2[f\"{i}\"]*dat2[f\"{i}\"]\\n    print(f\"{i}{i}\")\\ndat2.columns'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for i in range(0,75):\n",
    "    dat2[f\"{i}{i}\"] = dat2[f\"{i}\"]*dat2[f\"{i}\"]\n",
    "    print(f\"{i}{i}\")\n",
    "dat2.columns'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corrmat = dat1[col].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(75):\n",
    "#    print(corrmat[corrmat[f'{i}']>0.5][f'{i}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(dat2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dat2 = dat2/dat2.max()\n",
    "#dat2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1[col] = dat1[col]/dat1[col].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dat1 = dat2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dat1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160000, 75) (40000, 75)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0    6\n",
       " 1    6\n",
       " 2    2\n",
       " 3    8\n",
       " 4    2\n",
       " Name: target, dtype: int64,\n",
       " 160000    1\n",
       " 160001    6\n",
       " 160002    3\n",
       " 160003    2\n",
       " 160004    2\n",
       " Name: target, dtype: int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat1.sample(frac=1)\n",
    "split = 160000\n",
    "#train = dat2[:split]\n",
    "#test = dat2[split:]\n",
    "x = dat1[:split][col]\n",
    "y = dat1[:split].target\n",
    "xt = dat1[split:][col]\n",
    "yt = dat1[split:].target\n",
    "print(x.shape, xt.shape)\n",
    "y.head(), yt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mytry = dat1[dat1[\\'0\\'] > 0].copy()\\nmytry = mytry[mytry[\\'target\\'] == 7]\\nprint(mytry[\\'0\\'].value_counts())\\nmytry = dat1[dat1[\\'1\\'] > 0].copy()\\nprint(mytry.target.value_counts())\\nfor i in range(2,75):\\n    feat = \"{}\"\\n    mytry = dat1[dat1[f\"{i}\"] == 0].copy()\\n    print(mytry.target.value_counts())'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''mytry = dat1[dat1['0'] > 0].copy()\n",
    "mytry = mytry[mytry['target'] == 7]\n",
    "print(mytry['0'].value_counts())\n",
    "mytry = dat1[dat1['1'] > 0].copy()\n",
    "print(mytry.target.value_counts())\n",
    "for i in range(2,75):\n",
    "    feat = \"{}\"\n",
    "    mytry = dat1[dat1[f\"{i}\"] == 0].copy()\n",
    "    print(mytry.target.value_counts())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mytry = dat[dat[\\'feature_0\\'] == 0].copy()\\nprint(mytry.target.value_counts())\\nmytry = dat[dat[\\'feature_1\\'] == 0].copy()\\nprint(mytry.target.value_counts())\\nfor i in range(2,75):\\n    feat = \"feature_{}\"\\n    mytry = dat[dat[f\"feature_{i}\"] == 0].copy()\\n    print(mytry.target.value_counts())'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''mytry = dat[dat['feature_0'] == 0].copy()\n",
    "print(mytry.target.value_counts())\n",
    "mytry = dat[dat['feature_1'] == 0].copy()\n",
    "print(mytry.target.value_counts())\n",
    "for i in range(2,75):\n",
    "    feat = \"feature_{}\"\n",
    "    mytry = dat[dat[f\"feature_{i}\"] == 0].copy()\n",
    "    print(mytry.target.value_counts())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0000, 0.0000, 0.0938, 0.0143, 0.0000, 0.0000, 0.0000, 0.0000, 0.1842,\n",
       "         0.0000, 0.0000, 0.0000, 0.0811, 0.0000, 0.0312, 0.0000, 0.0000, 0.2143,\n",
       "         0.1364, 0.0038, 0.0000, 0.0606, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0095, 0.0000, 0.0000, 0.0256, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.3235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1429, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0536, 0.0000, 0.0263, 0.0833,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0125, 0.0098,\n",
       "         0.0000, 0.0000, 0.0556, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0328, 0.0000, 0.0000]),\n",
       " tensor(5))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_x = torch.tensor(x.values.astype(np.float32))\n",
    "#train_x.add_(1)\n",
    "train_y = torch.tensor(y.values.astype(np.int64))\n",
    "train_y.add_(-1)\n",
    "#train_y = torch.nn.functional.one_hot(train_y)\n",
    "train_x[0], train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0000, 0.0196, 0.2031, 0.1143, 0.0000, 0.0658, 0.0233, 0.0000, 0.0526,\n",
       "         0.0000, 0.0303, 0.0000, 0.0000, 0.0000, 0.0312, 0.0000, 0.0000, 0.0000,\n",
       "         0.3182, 0.0000, 0.0333, 0.0000, 0.0000, 0.1818, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0256, 0.0641, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0294, 0.0000, 0.1224, 0.0000, 0.0274, 0.0000, 0.0159, 0.0741,\n",
       "         0.0000, 0.0000, 0.0309, 0.0000, 0.1316, 0.0000, 0.0137, 0.0526, 0.0278,\n",
       "         0.0000, 0.0000, 0.0652, 0.0000, 0.0333, 0.0028, 0.0043, 0.0250, 0.0588,\n",
       "         0.0000, 0.2800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0667,\n",
       "         0.0656, 0.3154, 0.0577]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = torch.tensor(xt.values.astype(np.float32))\n",
    "#test_x.add_(1)\n",
    "test_y = torch.tensor(yt.values.astype(np.int64))\n",
    "test_y.add_(-1)\n",
    "#test_y = torch.nn.functional.one_hot(test_y)\n",
    "test_x[0], test_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.mydataset at 0x16991a889d0>,\n",
       " <__main__.mydataset at 0x16991a88640>,\n",
       " (tensor([0.0000, 0.0000, 0.0938, 0.0143, 0.0000, 0.0000, 0.0000, 0.0000, 0.1842,\n",
       "          0.0000, 0.0000, 0.0000, 0.0811, 0.0000, 0.0312, 0.0000, 0.0000, 0.2143,\n",
       "          0.1364, 0.0038, 0.0000, 0.0606, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0095, 0.0000, 0.0000, 0.0256, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.3235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1429, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0536, 0.0000, 0.0263, 0.0833,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0125, 0.0098,\n",
       "          0.0000, 0.0000, 0.0556, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0328, 0.0000, 0.0000]),\n",
       "  tensor(5)),\n",
       " (tensor([0.0000, 0.0000, 0.0938, 0.0143, 0.0000, 0.0000, 0.0000, 0.0000, 0.1842,\n",
       "          0.0000, 0.0000, 0.0000, 0.0811, 0.0000, 0.0312, 0.0000, 0.0000, 0.2143,\n",
       "          0.1364, 0.0038, 0.0000, 0.0606, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0095, 0.0000, 0.0000, 0.0256, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.3235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1429, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0536, 0.0000, 0.0263, 0.0833,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0125, 0.0098,\n",
       "          0.0000, 0.0000, 0.0556, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0328, 0.0000, 0.0000]),\n",
       "  tensor(5)))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class mydataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.labels = labels\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self,idx):\n",
    "        item = self.data[idx]\n",
    "        #label = torch.nn.functional.one_hot(self.labels[idx],num_classes=9)\n",
    "        label = self.labels[idx]\n",
    "        return (item,label)\n",
    "training_data = mydataset(train_x,train_y)\n",
    "testing_data = mydataset(test_x,test_y)\n",
    "training_data, testing_data, next(iter(training_data)), training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(training_data, batch_size = 100, shuffle = True)\n",
    "test_dataloader = DataLoader(testing_data, batch_size = 100, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from torch import nn\\n\\nclass myloss(nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n    def forward(self,x,y):\\n        l1 = len(x)\\n        l = len(x[0])\\n        s = ((-1)*sum(y[0]*torch.log(x[0]) + (1-y[0])*(torch.log(1-x[0])))/l).clone()\\n        for X,Y in zip(x,y):\\n            s += ((-1)*sum(Y*torch.log(X) + (1-Y)*(torch.log(1-X)))/l).clone()\\n        s = s/l1\\n        return s\\na = myloss()\\na([torch.tensor([.3,.3,.3]),torch.tensor([.3,.3,.3])],[torch.tensor([0,0,1]),torch.tensor([0,0,1])])'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from torch import nn\n",
    "\n",
    "class myloss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self,x,y):\n",
    "        l1 = len(x)\n",
    "        l = len(x[0])\n",
    "        s = ((-1)*sum(y[0]*torch.log(x[0]) + (1-y[0])*(torch.log(1-x[0])))/l).clone()\n",
    "        for X,Y in zip(x,y):\n",
    "            s += ((-1)*sum(Y*torch.log(X) + (1-Y)*(torch.log(1-X)))/l).clone()\n",
    "        s = s/l1\n",
    "        return s\n",
    "a = myloss()\n",
    "a([torch.tensor([.3,.3,.3]),torch.tensor([.3,.3,.3])],[torch.tensor([0,0,1]),torch.tensor([0,0,1])])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork,self).__init__()\n",
    "        self.seq_modules = nn.Sequential(\n",
    "            nn.Linear(75,100),\n",
    "            nn.BatchNorm1d(num_features = 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(100,45),\n",
    "            nn.BatchNorm1d(num_features = 45),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(45,9),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        logits = self.seq_modules(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork()\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#pred = model(next(iter(train_dataloader)))\n",
    "#pred, pred.argmax(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  6.3639,   9.2108,  -0.7271,  ...,   7.7133,  -5.9643,   2.0096],\n",
      "        [-11.4032,  -0.3456,  -1.9172,  ...,   1.8718,  -9.4891,  -0.7950],\n",
      "        [ -8.1252,  -2.5892,  -0.0675,  ..., -10.4712,   2.9093,   0.9734],\n",
      "        ...,\n",
      "        [  3.2701,   9.4112,  -0.8646,  ...,   4.4504,   1.9496,  10.5874],\n",
      "        [ -6.6690,  -0.2522,  -3.2076,  ..., -11.4013,  -5.2081,  -5.7904],\n",
      "        [ -8.1135,   8.6788,   1.0343,  ...,   9.3495,   3.5315,   4.0280]],\n",
      "       grad_fn=<MulBackward1>)\n",
      "tensor([  8.0487,  -8.2219,   7.2746,  -0.6441,  10.0946,  11.1216,  -7.7319,\n",
      "          7.7818,   7.7496,  -0.4476,  -9.5909,   1.5685,  -7.3043,  11.1588,\n",
      "         -3.3742,  -5.8377,  -3.0987,  -8.4508,   7.6702,   1.4493,  -3.0463,\n",
      "         -8.3037,  -7.3122,   6.8028,  -1.1427,   2.5066,  -5.0382,   9.8125,\n",
      "         -5.8034,  -0.6905,   8.0432,  -9.6876,  -6.8563, -10.8590,   2.0620,\n",
      "         -6.6833, -10.3072,   4.1408,  -7.1085,  -0.6839,   2.7290,  -5.1677,\n",
      "         -5.7561,  -1.6961,   4.6361,   6.1953,   7.2032,   4.0684,   4.9709,\n",
      "         -7.3938,  -5.0972,  -2.5750,   1.9727,  -7.6205,   3.2684,   0.7113,\n",
      "         -2.0317,   0.1910,  -0.7040,   5.6922,   3.9983,   0.9007,   4.6494,\n",
      "          8.0830,  -5.3708,   8.8581,  -4.0682,   0.4539,   5.8874,  -3.1244,\n",
      "         -9.5756,   2.6605,  10.7612,   3.9130,  -3.7156,   7.8735,   1.7740,\n",
      "         -7.2663,  -1.4653,  -8.4193,   1.1419,   6.3765, -11.1271, -10.0274,\n",
      "          5.2179,  -5.9762,   6.1153,  11.1485,  -9.7619,   1.7806,  -7.2249,\n",
      "          5.1981,  -4.5204,  -4.6964,   3.6724, -10.7555,  -7.4660, -10.3666,\n",
      "          5.9963,   5.3440], grad_fn=<MulBackward1>)\n",
      "tensor([100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100.,\n",
      "        100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100.,\n",
      "        100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100.,\n",
      "        100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100.,\n",
      "        100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100.,\n",
      "        100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100.,\n",
      "        100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100.,\n",
      "        100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100.,\n",
      "        100., 100., 100., 100.], grad_fn=<MulBackward1>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.], grad_fn=<MulBackward1>)\n",
      "tensor([[-6.5791,  6.1688,  4.1120,  ..., -9.1093,  2.7455, -0.7788],\n",
      "        [ 8.0639, -1.9682,  8.3566,  ..., -2.5572, -3.2698, -8.4970],\n",
      "        [ 5.2274, -2.7361, -4.1528,  ...,  4.4899, -0.5853,  6.1995],\n",
      "        ...,\n",
      "        [ 5.0050,  6.2631, -6.8986,  ...,  0.1573, -3.8022, -9.2711],\n",
      "        [ 3.9972,  3.6593, -0.9997,  ...,  6.6962,  4.0588,  9.5982],\n",
      "        [-6.6573, -7.4699,  3.7757,  ..., -4.4505,  9.5400,  0.9746]],\n",
      "       grad_fn=<MulBackward1>)\n",
      "tensor([-9.6594,  7.7621, -7.6360, -3.2898,  4.1252, -6.7090,  2.1930, -6.7453,\n",
      "        -7.5354, -5.3115,  6.2058,  9.9010,  7.8480, -9.2480, -4.3509,  4.3256,\n",
      "         5.3369, -2.3408, -9.1887, -0.5935,  7.2846, -6.7688, -6.2970,  9.3566,\n",
      "        -3.2027,  1.9356, -3.7089,  4.8829, -0.6822,  7.1921,  2.7451,  5.3166,\n",
      "         4.7069, -1.6834, -9.4324, -4.9417, -9.7902, -9.2887,  8.2051,  4.0034,\n",
      "        -4.2435,  1.2047,  2.5694,  8.1761,  9.7650], grad_fn=<MulBackward1>)\n",
      "tensor([100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100.,\n",
      "        100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100.,\n",
      "        100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100.,\n",
      "        100., 100., 100., 100., 100., 100., 100., 100., 100.],\n",
      "       grad_fn=<MulBackward1>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       grad_fn=<MulBackward1>)\n",
      "tensor([[  5.2912, -14.5806, -10.3365,  -6.0118, -13.5969, -14.4941, -11.5703,\n",
      "           3.7541,   9.8747,   5.9808,  10.8413,   6.7778,  -6.8720,  14.4493,\n",
      "          -9.5560,   5.9732,   2.3618,   8.3505,  12.7200,   9.0285,  -4.9861,\n",
      "          -6.4401,   4.5831,   6.8517,   1.2710,   4.9426,   1.4555,   1.9674,\n",
      "         -12.2532,  10.8682, -10.5917, -14.3893,   0.9685,   2.6223,   9.6959,\n",
      "          -6.0717,   9.5269,  13.7000,  -1.6966, -10.9678,  13.5160,  -6.1105,\n",
      "          -4.6858,   8.9504,   8.4089],\n",
      "        [  6.7458,   2.7413,   3.3727, -13.4340,   2.6032,  14.5513, -12.4900,\n",
      "           7.4358,   6.9606,  -0.8201,   3.4707,  12.5264, -11.6289,   1.4699,\n",
      "         -13.4852,  -8.2463,  -9.9949,  14.7853,  12.1889,  11.6775,  -9.0311,\n",
      "          -2.6331,  -6.9237,  12.3695,  -8.8989,  -3.4844,   5.1409,  13.5062,\n",
      "          14.2308,  -2.1932,   3.6713,  -5.3305,  10.8206, -11.7748,   6.5486,\n",
      "           4.2055,   1.9890, -10.1169,   3.1376,   7.6593,   6.2251,  -5.6342,\n",
      "           4.8874,  13.3989,   8.5531],\n",
      "        [ -6.3842,   5.0653,   1.5702,   0.5314,  -8.3914, -13.5984,  -7.6225,\n",
      "          -1.6866,   2.8940,   9.3124,  -2.5232, -14.3961, -12.6646,   9.7400,\n",
      "           2.0074,  -5.0273, -14.6626,  -2.1986,  -7.2897,  -5.3596, -14.7195,\n",
      "          -5.2474,  -3.5103,   2.8865,  -8.1541, -14.8400,  -9.8698,  -4.1656,\n",
      "           5.6688,   9.4628,  -2.8624,  -0.6155,   3.3873, -11.8856,   0.3804,\n",
      "         -13.3495,  -2.9965,   6.0768,  -5.2022,  13.1825,  13.1618,  -5.0986,\n",
      "          14.5187,  14.1208,  -4.3391],\n",
      "        [ -3.2468,   7.4452,   7.8516,  -0.6719, -14.8861,   3.4995,   4.8883,\n",
      "          -4.0948,  12.7022,  14.8741,  11.4420,   7.9335, -10.2322, -10.9444,\n",
      "          14.0901,  12.6928, -11.0292,  -5.2831,  -7.7889,  -5.1590,  14.4057,\n",
      "           3.3034,   3.5751,  -4.9753,   0.2876,   6.5371,  -4.3652,  -0.3879,\n",
      "          -5.4817,  -9.7030,   2.5000,  14.4086,   4.5678,  -8.1035,  -9.4357,\n",
      "           3.8842,   0.0744, -14.7543, -11.4102,   4.4611,  -2.9506,   8.9678,\n",
      "          10.8160, -10.2877,  -5.6648],\n",
      "        [  7.0064,  12.3437,  12.5251,   3.2396,  14.4985,  -2.9999,  14.7411,\n",
      "          10.6186, -10.2275,  -4.8236,  -5.2562,  12.9043,   8.9521,   5.6632,\n",
      "           5.6465,  -8.6673,   1.1799,  -3.0697,  -0.8920,  13.2951,  -0.8438,\n",
      "           7.1749,  -7.8938,  -8.6911,   0.0788,  -0.0546,  -9.2586,  -9.4235,\n",
      "           7.8793, -13.4585, -14.8473,  13.5889,  -2.3979,   6.2327,   5.9743,\n",
      "         -10.3272,   6.4947,   9.1992,  13.4321,  -9.7585,   5.5374,   2.9220,\n",
      "           7.4464,  -8.9083,  -0.9067],\n",
      "        [ 13.1503,   1.8321,   0.8000,   2.1468,  -6.4309,  -2.3344, -13.8539,\n",
      "           0.8594, -13.3466,  -5.8834,  -8.1220,   9.2141,   2.5074,  -8.9667,\n",
      "           7.1463,   2.2792,  11.1599, -11.4526,   8.0977, -13.3093,   2.1570,\n",
      "         -13.9359,  -4.2153,  -1.7685,  -0.7755,   5.3618,   0.1181,  13.6533,\n",
      "          12.2190, -13.4647, -14.6920, -11.6736,  10.0605,   1.7810,  -5.8822,\n",
      "          -9.3498,   3.8682,   5.2542,   3.0590,  10.1653, -10.9981, -14.7848,\n",
      "          -3.8663,  13.1032,  14.1955],\n",
      "        [ 12.2227,  12.5256,  13.9533,  -5.2496,   6.0033,   3.5965,   6.9219,\n",
      "           3.3491,  -9.9654,   0.4541,   3.3431,  -2.4938,  -8.1491,  10.5414,\n",
      "          11.3181,  10.7611, -14.3895,  -9.4844,  -2.3808, -13.7666,  12.3758,\n",
      "          14.0773,   2.0229,   2.4409,  -5.1402,  13.6802,  -1.2287,  11.6586,\n",
      "          -6.5390,  10.3126,   0.4618,  13.1862,   5.4787,  -7.0707,  10.3054,\n",
      "           2.7302,  -0.2340,  -5.5210,  13.8332,  -2.4999,   6.5803, -13.6372,\n",
      "           4.1811,  -0.4200,  -2.7949],\n",
      "        [  1.9281,   2.4810, -13.7032,  -9.2770,  13.3775,  -6.8171, -13.9766,\n",
      "          -5.8416,  13.7255,  -1.3037,  -1.4899,   6.4193,   1.7795,   5.8080,\n",
      "           5.1724, -13.1818,  13.3451,   8.7209, -10.1499, -13.4321,  -0.8359,\n",
      "          12.1095,   1.7055,   0.9186,  -5.8828,   7.6410, -13.6913,  14.1003,\n",
      "          12.8998,   0.0880,   4.7083,   5.4372,  -2.7924,  -2.6299,   9.1097,\n",
      "           1.7226,  12.1169,  -1.9807,   9.0898,  -7.0652,  13.6933,  14.8923,\n",
      "           9.5292,  -5.9132,   0.5084],\n",
      "        [  9.2278,  10.6104,  -0.6423,  -0.1647,  10.5887,  12.2734,  -9.6189,\n",
      "          -6.7644,   8.5783, -10.7978,   1.1418,   4.8576,   7.5038,  -6.4414,\n",
      "          -1.2104,  -4.2370, -11.3732, -11.6933,  11.0010, -12.5116, -11.3856,\n",
      "          -9.1910,  12.1031,   0.4849,  11.8973,  -5.1037,  -8.3480,   2.0318,\n",
      "           5.4513,  -9.2342, -12.6487,  10.1280, -10.7068,   3.9581,  10.2966,\n",
      "          -8.7866,  -7.1234,  10.5779,  -3.1921,   8.8301,  -9.0060,   4.6160,\n",
      "           6.4289,  -5.3794,  -9.8888]], grad_fn=<MulBackward1>)\n",
      "tensor([-10.8520,   2.5907,  13.2550,  -0.4908,  -5.2981, -12.6885,  -6.8618,\n",
      "          5.6684,  -6.7203], grad_fn=<MulBackward1>)\n"
     ]
    }
   ],
   "source": [
    "for x in model.parameters():\n",
    "    x = x.multiply(100)\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimiser):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        correct += ((nn.functional.softmax(pred)).argmax(1) == y).type(torch.float).sum().item()\n",
    "        \n",
    "        if (batch+1)%200 == 0 :\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}], accuracy = {correct*100/current}\")\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += ((nn.functional.softmax(pred)).argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-86-f161d4b08ec4>:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  correct += ((nn.functional.softmax(pred)).argmax(1) == y).type(torch.float).sum().item()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.704190  [ 9950/160000], accuracy = 36.11055276381909\n",
      "loss: 1.567688  [19950/160000], accuracy = 35.45363408521303\n",
      "loss: 1.841494  [29950/160000], accuracy = 35.62938230383973\n",
      "loss: 1.807764  [39950/160000], accuracy = 35.677096370463076\n",
      "loss: 1.937318  [49950/160000], accuracy = 35.68368368368368\n",
      "loss: 1.473621  [59950/160000], accuracy = 35.7581317764804\n",
      "loss: 1.670188  [69950/160000], accuracy = 35.64974982130093\n",
      "loss: 1.835750  [79950/160000], accuracy = 35.55222013758599\n",
      "loss: 1.594219  [89950/160000], accuracy = 35.52973874374653\n",
      "loss: 1.780905  [99950/160000], accuracy = 35.49874937468734\n",
      "loss: 1.601875  [109950/160000], accuracy = 35.47794452023647\n",
      "loss: 1.602077  [119950/160000], accuracy = 35.485619007919965\n",
      "loss: 1.850824  [129950/160000], accuracy = 35.455944594074644\n",
      "loss: 1.681271  [139950/160000], accuracy = 35.464094319399784\n",
      "loss: 1.775392  [149950/160000], accuracy = 35.451150383461155\n",
      "loss: 1.733500  [159950/160000], accuracy = 35.43232260081275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-86-f161d4b08ec4>:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  correct += ((nn.functional.softmax(pred)).argmax(1) == y).type(torch.float).sum().item()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 0.035684 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.727819  [ 9950/160000], accuracy = 36.33165829145729\n",
      "loss: 1.531716  [19950/160000], accuracy = 36.34586466165413\n",
      "loss: 1.844967  [29950/160000], accuracy = 36.0567612687813\n",
      "loss: 1.693398  [39950/160000], accuracy = 36.05006257822278\n",
      "loss: 1.693697  [49950/160000], accuracy = 35.9019019019019\n",
      "loss: 1.780951  [59950/160000], accuracy = 35.83819849874896\n",
      "loss: 1.839656  [69950/160000], accuracy = 35.7855611150822\n",
      "loss: 1.710618  [79950/160000], accuracy = 35.67729831144465\n",
      "loss: 1.775517  [89950/160000], accuracy = 35.662034463590885\n",
      "loss: 1.969712  [99950/160000], accuracy = 35.656828414207105\n",
      "loss: 1.521127  [109950/160000], accuracy = 35.598908594815825\n",
      "loss: 1.754414  [119950/160000], accuracy = 35.582325969153814\n",
      "loss: 1.551610  [129950/160000], accuracy = 35.57983839938438\n",
      "loss: 1.799732  [139950/160000], accuracy = 35.591282600928906\n",
      "loss: 1.620532  [149950/160000], accuracy = 35.5838612870957\n",
      "loss: 2.013132  [159950/160000], accuracy = 35.563613629259144\n",
      "Test Error: \n",
      " Accuracy: 35.0%, Avg loss: 0.035714 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.830731  [ 9950/160000], accuracy = 35.256281407035175\n",
      "loss: 1.705699  [19950/160000], accuracy = 35.3984962406015\n",
      "loss: 1.784653  [29950/160000], accuracy = 35.62938230383973\n",
      "loss: 1.657112  [39950/160000], accuracy = 35.5244055068836\n",
      "loss: 1.920464  [49950/160000], accuracy = 35.51951951951952\n",
      "loss: 1.831189  [59950/160000], accuracy = 35.57464553794829\n",
      "loss: 1.745349  [69950/160000], accuracy = 35.598284488920655\n",
      "loss: 1.474198  [79950/160000], accuracy = 35.53846153846154\n",
      "loss: 2.027490  [89950/160000], accuracy = 35.62757087270706\n",
      "loss: 1.659305  [99950/160000], accuracy = 35.618809404702354\n",
      "loss: 1.803609  [109950/160000], accuracy = 35.617098681218735\n",
      "loss: 1.785289  [119950/160000], accuracy = 35.52563568153397\n",
      "loss: 1.790671  [129950/160000], accuracy = 35.54290111581378\n",
      "loss: 1.586534  [139950/160000], accuracy = 35.486959628438726\n",
      "loss: 1.626031  [149950/160000], accuracy = 35.454484828276094\n",
      "loss: 1.762644  [159950/160000], accuracy = 35.44545170365739\n",
      "Test Error: \n",
      " Accuracy: 35.0%, Avg loss: 0.035703 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.623434  [ 9950/160000], accuracy = 34.78391959798995\n",
      "loss: 1.869662  [19950/160000], accuracy = 35.203007518796994\n",
      "loss: 1.880288  [29950/160000], accuracy = 35.44908180300501\n",
      "loss: 1.658513  [39950/160000], accuracy = 35.5944931163955\n",
      "loss: 1.839963  [49950/160000], accuracy = 35.769769769769766\n",
      "loss: 1.898664  [59950/160000], accuracy = 35.799833194328606\n",
      "loss: 1.808276  [69950/160000], accuracy = 35.7169406719085\n",
      "loss: 1.597629  [79950/160000], accuracy = 35.6635397123202\n",
      "loss: 1.766103  [89950/160000], accuracy = 35.57087270705948\n",
      "loss: 1.663017  [99950/160000], accuracy = 35.45672836418209\n",
      "loss: 1.694746  [109950/160000], accuracy = 35.41518872214643\n",
      "loss: 1.713145  [119950/160000], accuracy = 35.36473530637766\n",
      "loss: 1.940215  [129950/160000], accuracy = 35.33282031550596\n",
      "loss: 1.668275  [139950/160000], accuracy = 35.33690603787067\n",
      "loss: 1.827454  [149950/160000], accuracy = 35.339113037679226\n",
      "loss: 1.813079  [159950/160000], accuracy = 35.34479524851516\n",
      "Test Error: \n",
      " Accuracy: 35.0%, Avg loss: 0.035694 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.922672  [ 9950/160000], accuracy = 35.66834170854271\n",
      "loss: 1.748954  [19950/160000], accuracy = 35.709273182957396\n",
      "loss: 1.792831  [29950/160000], accuracy = 35.53923205342237\n",
      "loss: 1.691136  [39950/160000], accuracy = 35.546933667083856\n",
      "loss: 1.975715  [49950/160000], accuracy = 35.431431431431434\n",
      "loss: 1.826062  [59950/160000], accuracy = 35.38115095913261\n",
      "loss: 1.580902  [69950/160000], accuracy = 35.4253037884203\n",
      "loss: 1.804445  [79950/160000], accuracy = 35.44090056285178\n",
      "loss: 2.026072  [89950/160000], accuracy = 35.36964980544747\n",
      "loss: 1.750113  [99950/160000], accuracy = 35.37468734367184\n",
      "loss: 1.779490  [109950/160000], accuracy = 35.4206457480673\n",
      "loss: 1.781359  [119950/160000], accuracy = 35.40225093789079\n",
      "loss: 1.892629  [129950/160000], accuracy = 35.350519430550214\n",
      "loss: 1.736270  [139950/160000], accuracy = 35.417649160414435\n",
      "loss: 1.759441  [149950/160000], accuracy = 35.457152384128044\n",
      "loss: 1.553077  [159950/160000], accuracy = 35.482338230697096\n",
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 0.035685 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.723130  [ 9950/160000], accuracy = 36.71356783919598\n",
      "loss: 1.740776  [19950/160000], accuracy = 36.015037593984964\n",
      "loss: 1.737393  [29950/160000], accuracy = 35.85976627712855\n",
      "loss: 1.576664  [39950/160000], accuracy = 35.67459324155194\n",
      "loss: 1.814991  [49950/160000], accuracy = 35.7957957957958\n",
      "loss: 1.869366  [59950/160000], accuracy = 35.86321934945788\n",
      "loss: 1.767721  [69950/160000], accuracy = 35.7240886347391\n",
      "loss: 2.057445  [79950/160000], accuracy = 35.647279549718576\n",
      "loss: 1.786141  [89950/160000], accuracy = 35.578654808226794\n",
      "loss: 1.542063  [99950/160000], accuracy = 35.59479739869935\n",
      "loss: 1.842702  [109950/160000], accuracy = 35.54615734424738\n",
      "loss: 1.910673  [119950/160000], accuracy = 35.44977073780742\n",
      "loss: 1.728791  [129950/160000], accuracy = 35.468257021931514\n",
      "loss: 1.887442  [139950/160000], accuracy = 35.44194355126831\n",
      "loss: 1.613344  [149950/160000], accuracy = 35.448482827609205\n",
      "loss: 1.813350  [159950/160000], accuracy = 35.47108471397312\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 0.035682 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.794616  [ 9950/160000], accuracy = 35.12562814070352\n",
      "loss: 1.758699  [19950/160000], accuracy = 35.73934837092732\n",
      "loss: 1.819816  [29950/160000], accuracy = 35.462437395659435\n",
      "loss: 1.914305  [39950/160000], accuracy = 35.26658322903629\n",
      "loss: 1.909450  [49950/160000], accuracy = 35.17917917917918\n",
      "loss: 1.806615  [59950/160000], accuracy = 35.17931609674729\n",
      "loss: 2.027230  [69950/160000], accuracy = 35.05503931379557\n",
      "loss: 1.895831  [79950/160000], accuracy = 35.184490306441525\n",
      "loss: 2.064497  [89950/160000], accuracy = 35.357420789327406\n",
      "loss: 1.661002  [99950/160000], accuracy = 35.321660830415205\n",
      "loss: 1.625637  [109950/160000], accuracy = 35.36152796725784\n",
      "loss: 1.965687  [119950/160000], accuracy = 35.36223426427678\n",
      "loss: 1.809075  [129950/160000], accuracy = 35.37514428626395\n",
      "loss: 1.893275  [139950/160000], accuracy = 35.37191854233655\n",
      "loss: 1.806779  [149950/160000], accuracy = 35.38579526508836\n",
      "loss: 1.608748  [159950/160000], accuracy = 35.412316348859015\n",
      "Test Error: \n",
      " Accuracy: 34.8%, Avg loss: 0.035704 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.857286  [ 9950/160000], accuracy = 35.85929648241206\n",
      "loss: 1.660388  [19950/160000], accuracy = 35.854636591478695\n",
      "loss: 1.812409  [29950/160000], accuracy = 35.7228714524207\n",
      "loss: 1.602812  [39950/160000], accuracy = 35.622027534418024\n",
      "loss: 1.921295  [49950/160000], accuracy = 35.609609609609606\n",
      "loss: 1.749581  [59950/160000], accuracy = 35.54295246038365\n",
      "loss: 2.007205  [69950/160000], accuracy = 35.63688348820586\n",
      "loss: 1.710306  [79950/160000], accuracy = 35.613508442776734\n",
      "loss: 1.714272  [89950/160000], accuracy = 35.66314619232907\n",
      "loss: 1.700144  [99950/160000], accuracy = 35.616808404202104\n",
      "loss: 1.705030  [109950/160000], accuracy = 35.57526148249204\n",
      "loss: 1.845483  [119950/160000], accuracy = 35.523134639433096\n",
      "loss: 1.749416  [129950/160000], accuracy = 35.41823778376298\n",
      "loss: 1.617104  [139950/160000], accuracy = 35.369774919614144\n",
      "loss: 1.719633  [149950/160000], accuracy = 35.358452817605865\n",
      "loss: 1.827878  [159950/160000], accuracy = 35.3579243513598\n",
      "Test Error: \n",
      " Accuracy: 35.0%, Avg loss: 0.035707 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.722792  [ 9950/160000], accuracy = 35.64824120603015\n",
      "loss: 1.843976  [19950/160000], accuracy = 35.3984962406015\n",
      "loss: 1.798645  [29950/160000], accuracy = 35.05175292153589\n",
      "loss: 1.830994  [39950/160000], accuracy = 35.05131414267835\n",
      "loss: 1.823049  [49950/160000], accuracy = 35.13913913913914\n",
      "loss: 1.604805  [59950/160000], accuracy = 35.311092577147626\n",
      "loss: 1.700157  [69950/160000], accuracy = 35.29664045746962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.626086  [79950/160000], accuracy = 35.31957473420888\n",
      "loss: 1.819114  [89950/160000], accuracy = 35.33629794330184\n",
      "loss: 1.970541  [99950/160000], accuracy = 35.31865932966483\n",
      "loss: 1.837649  [109950/160000], accuracy = 35.32514779445202\n",
      "loss: 1.741786  [119950/160000], accuracy = 35.38557732388495\n",
      "loss: 1.684084  [129950/160000], accuracy = 35.41362062331666\n",
      "loss: 1.781627  [139950/160000], accuracy = 35.45051804215792\n",
      "loss: 1.634958  [149950/160000], accuracy = 35.56852284094698\n",
      "loss: 1.807820  [159950/160000], accuracy = 35.53610503282276\n",
      "Test Error: \n",
      " Accuracy: 35.0%, Avg loss: 0.035676 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.791749  [ 9950/160000], accuracy = 36.2713567839196\n",
      "loss: 1.779868  [19950/160000], accuracy = 35.68421052631579\n",
      "loss: 1.694199  [29950/160000], accuracy = 35.86644407345576\n",
      "loss: 1.636151  [39950/160000], accuracy = 35.58698372966208\n",
      "loss: 1.746919  [49950/160000], accuracy = 35.64364364364364\n",
      "loss: 1.783973  [59950/160000], accuracy = 35.512927439532945\n",
      "loss: 1.530198  [69950/160000], accuracy = 35.55253752680486\n",
      "loss: 2.118383  [79950/160000], accuracy = 35.51344590368981\n",
      "loss: 1.954677  [89950/160000], accuracy = 35.494163424124515\n",
      "loss: 1.588639  [99950/160000], accuracy = 35.50075037518759\n",
      "loss: 1.643463  [109950/160000], accuracy = 35.49067758071851\n",
      "loss: 1.717548  [119950/160000], accuracy = 35.41892455189662\n",
      "loss: 1.789298  [129950/160000], accuracy = 35.46056175452097\n",
      "loss: 1.709724  [139950/160000], accuracy = 35.449088960342976\n",
      "loss: 1.705333  [149950/160000], accuracy = 35.450483494498165\n",
      "loss: 1.776102  [159950/160000], accuracy = 35.461706783369806\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 0.035714 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.553841  [ 9950/160000], accuracy = 35.57788944723618\n",
      "loss: 1.915271  [19950/160000], accuracy = 35.43859649122807\n",
      "loss: 1.719363  [29950/160000], accuracy = 35.48580968280467\n",
      "loss: 1.686867  [39950/160000], accuracy = 35.339173967459324\n",
      "loss: 1.534287  [49950/160000], accuracy = 35.35535535535536\n",
      "loss: 1.791035  [59950/160000], accuracy = 35.4395329441201\n",
      "loss: 1.838034  [69950/160000], accuracy = 35.36954967834167\n",
      "loss: 1.754188  [79950/160000], accuracy = 35.41963727329581\n",
      "loss: 2.094118  [89950/160000], accuracy = 35.45414118954975\n",
      "loss: 1.777762  [99950/160000], accuracy = 35.39169584792396\n",
      "loss: 1.845450  [109950/160000], accuracy = 35.41882673942701\n",
      "loss: 1.590175  [119950/160000], accuracy = 35.45477282200917\n",
      "loss: 1.871745  [129950/160000], accuracy = 35.47441323585995\n",
      "loss: 1.771407  [139950/160000], accuracy = 35.44408717399071\n",
      "loss: 1.732363  [149950/160000], accuracy = 35.42447482494165\n",
      "loss: 1.894017  [159950/160000], accuracy = 35.413566739606125\n",
      "Test Error: \n",
      " Accuracy: 35.0%, Avg loss: 0.035685 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.679832  [ 9950/160000], accuracy = 36.33165829145729\n",
      "loss: 1.619712  [19950/160000], accuracy = 35.93984962406015\n",
      "loss: 1.865210  [29950/160000], accuracy = 35.62604340567613\n",
      "loss: 1.918353  [39950/160000], accuracy = 35.339173967459324\n",
      "loss: 1.872722  [49950/160000], accuracy = 35.35935935935936\n",
      "loss: 1.812284  [59950/160000], accuracy = 35.367806505421186\n",
      "loss: 1.885684  [69950/160000], accuracy = 35.37097927090779\n",
      "loss: 1.725605  [79950/160000], accuracy = 35.31957473420888\n",
      "loss: 1.769978  [89950/160000], accuracy = 35.30405780989439\n",
      "loss: 1.593420  [99950/160000], accuracy = 35.38869434717358\n",
      "loss: 1.754277  [109950/160000], accuracy = 35.39517962710323\n",
      "loss: 1.861987  [119950/160000], accuracy = 35.45310546060859\n",
      "loss: 1.673158  [129950/160000], accuracy = 35.42824163139669\n",
      "loss: 1.915631  [139950/160000], accuracy = 35.436227224008576\n",
      "loss: 1.794652  [149950/160000], accuracy = 35.49983327775925\n",
      "loss: 1.588771  [159950/160000], accuracy = 35.50609565489216\n",
      "Test Error: \n",
      " Accuracy: 35.0%, Avg loss: 0.035721 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.697775  [ 9950/160000], accuracy = 35.49748743718593\n",
      "loss: 1.712817  [19950/160000], accuracy = 35.578947368421055\n",
      "loss: 1.920440  [29950/160000], accuracy = 35.38230383973289\n",
      "loss: 1.789023  [39950/160000], accuracy = 35.41677096370463\n",
      "loss: 1.910230  [49950/160000], accuracy = 35.46346346346346\n",
      "loss: 1.647249  [59950/160000], accuracy = 35.56797331109258\n",
      "loss: 1.924473  [69950/160000], accuracy = 35.70836311651179\n",
      "loss: 1.774972  [79950/160000], accuracy = 35.64102564102564\n",
      "loss: 1.694808  [89950/160000], accuracy = 35.598665925514176\n",
      "loss: 1.778298  [99950/160000], accuracy = 35.51875937968985\n",
      "loss: 1.851092  [109950/160000], accuracy = 35.54797635288767\n",
      "loss: 1.658207  [119950/160000], accuracy = 35.54147561483952\n",
      "loss: 1.713944  [129950/160000], accuracy = 35.496729511350516\n",
      "loss: 1.811907  [139950/160000], accuracy = 35.44551625580564\n",
      "loss: 1.561310  [149950/160000], accuracy = 35.48049349783261\n",
      "loss: 1.707755  [159950/160000], accuracy = 35.42294467020944\n",
      "Test Error: \n",
      " Accuracy: 35.0%, Avg loss: 0.035682 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.734908  [ 9950/160000], accuracy = 35.7286432160804\n",
      "loss: 1.770236  [19950/160000], accuracy = 35.5187969924812\n",
      "loss: 1.566865  [29950/160000], accuracy = 35.48580968280467\n",
      "loss: 1.936078  [39950/160000], accuracy = 35.42177722152691\n",
      "loss: 1.756021  [49950/160000], accuracy = 35.451451451451454\n",
      "loss: 1.745001  [59950/160000], accuracy = 35.42618849040868\n",
      "loss: 1.962292  [69950/160000], accuracy = 35.442458899213726\n",
      "loss: 1.811317  [79950/160000], accuracy = 35.46091307066917\n",
      "loss: 1.765726  [89950/160000], accuracy = 35.38854919399667\n",
      "loss: 1.778159  [99950/160000], accuracy = 35.431715857928964\n",
      "loss: 1.736319  [109950/160000], accuracy = 35.485220554797635\n",
      "loss: 1.674763  [119950/160000], accuracy = 35.52396832013339\n",
      "loss: 1.929087  [129950/160000], accuracy = 35.49595998460946\n",
      "loss: 2.098823  [139950/160000], accuracy = 35.47052518756699\n",
      "loss: 1.524958  [149950/160000], accuracy = 35.41713904634878\n",
      "loss: 1.740228  [159950/160000], accuracy = 35.46795873710535\n",
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 0.035695 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.755305  [ 9950/160000], accuracy = 35.22613065326633\n",
      "loss: 1.813333  [19950/160000], accuracy = 34.92731829573935\n",
      "loss: 1.775142  [29950/160000], accuracy = 35.38230383973289\n",
      "loss: 1.899332  [39950/160000], accuracy = 35.38423028785982\n",
      "loss: 1.865783  [49950/160000], accuracy = 35.429429429429426\n",
      "loss: 1.613755  [59950/160000], accuracy = 35.49958298582152\n",
      "loss: 1.602857  [69950/160000], accuracy = 35.41529664045747\n",
      "loss: 1.789122  [79950/160000], accuracy = 35.34834271419637\n",
      "loss: 1.654552  [89950/160000], accuracy = 35.30961645358533\n",
      "loss: 1.713139  [99950/160000], accuracy = 35.326663331665834\n",
      "loss: 1.933679  [109950/160000], accuracy = 35.36334697589813\n",
      "loss: 1.905725  [119950/160000], accuracy = 35.31388078365986\n",
      "loss: 1.694359  [129950/160000], accuracy = 35.30896498653328\n",
      "loss: 1.678136  [139950/160000], accuracy = 35.30189353340479\n",
      "loss: 1.729321  [149950/160000], accuracy = 35.32644214738246\n",
      "loss: 1.728168  [159950/160000], accuracy = 35.29728040012504\n",
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 0.035715 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.804632  [ 9950/160000], accuracy = 35.58793969849246\n",
      "loss: 1.724354  [19950/160000], accuracy = 35.734335839599\n",
      "loss: 1.853988  [29950/160000], accuracy = 35.8330550918197\n",
      "loss: 1.794630  [39950/160000], accuracy = 35.68210262828536\n",
      "loss: 1.837659  [49950/160000], accuracy = 35.53553553553554\n",
      "loss: 1.711276  [59950/160000], accuracy = 35.694745621351124\n",
      "loss: 1.935976  [69950/160000], accuracy = 35.56254467476769\n",
      "loss: 1.748655  [79950/160000], accuracy = 35.590994371482175\n",
      "loss: 1.630609  [89950/160000], accuracy = 35.55530850472485\n",
      "loss: 1.638391  [99950/160000], accuracy = 35.52176088044022\n",
      "loss: 1.842222  [109950/160000], accuracy = 35.525238744884035\n",
      "loss: 1.923157  [119950/160000], accuracy = 35.531471446436015\n",
      "loss: 1.769631  [129950/160000], accuracy = 35.48749519045787\n",
      "loss: 1.699316  [139950/160000], accuracy = 35.45909253304752\n",
      "loss: 1.633221  [149950/160000], accuracy = 35.47849283094365\n",
      "loss: 1.992244  [159950/160000], accuracy = 35.46545795561113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 0.035684 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.682165  [ 9950/160000], accuracy = 34.97487437185929\n",
      "loss: 1.756493  [19950/160000], accuracy = 35.38345864661654\n",
      "loss: 1.895227  [29950/160000], accuracy = 35.66277128547579\n",
      "loss: 1.820133  [39950/160000], accuracy = 35.61952440550689\n",
      "loss: 2.034138  [49950/160000], accuracy = 35.5055055055055\n",
      "loss: 1.752428  [59950/160000], accuracy = 35.59799833194329\n",
      "loss: 1.704688  [69950/160000], accuracy = 35.572551822730524\n",
      "loss: 1.513152  [79950/160000], accuracy = 35.54471544715447\n",
      "loss: 1.712888  [89950/160000], accuracy = 35.47192884936076\n",
      "loss: 1.859655  [99950/160000], accuracy = 35.509754877438716\n",
      "loss: 1.594464  [109950/160000], accuracy = 35.49522510231924\n",
      "loss: 1.832587  [119950/160000], accuracy = 35.49895789912463\n",
      "loss: 1.976356  [129950/160000], accuracy = 35.472104655636784\n",
      "loss: 1.799374  [139950/160000], accuracy = 35.47552697391926\n",
      "loss: 1.659716  [149950/160000], accuracy = 35.43181060353451\n",
      "loss: 1.778143  [159950/160000], accuracy = 35.40231322288215\n",
      "Test Error: \n",
      " Accuracy: 35.0%, Avg loss: 0.035714 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.945043  [ 9950/160000], accuracy = 35.15577889447236\n",
      "loss: 1.479522  [19950/160000], accuracy = 35.11779448621554\n",
      "loss: 1.844929  [29950/160000], accuracy = 35.18530884808013\n",
      "loss: 1.869701  [39950/160000], accuracy = 35.4468085106383\n",
      "loss: 1.864281  [49950/160000], accuracy = 35.55755755755756\n",
      "loss: 1.845569  [59950/160000], accuracy = 35.419516263552964\n",
      "loss: 1.537567  [69950/160000], accuracy = 35.52394567548249\n",
      "loss: 1.820934  [79950/160000], accuracy = 35.563477173233274\n",
      "loss: 1.864378  [89950/160000], accuracy = 35.57087270705948\n",
      "loss: 1.824686  [99950/160000], accuracy = 35.49074537268634\n",
      "loss: 1.798805  [109950/160000], accuracy = 35.43792633015007\n",
      "loss: 1.696564  [119950/160000], accuracy = 35.46727803251355\n",
      "loss: 1.753313  [129950/160000], accuracy = 35.48287803001154\n",
      "loss: 1.824752  [139950/160000], accuracy = 35.486959628438726\n",
      "loss: 1.555409  [149950/160000], accuracy = 35.48916305435145\n",
      "loss: 1.787035  [159950/160000], accuracy = 35.497968115035945\n",
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 0.035683 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.685899  [ 9950/160000], accuracy = 35.256281407035175\n",
      "loss: 1.578936  [19950/160000], accuracy = 35.49874686716792\n",
      "loss: 1.676177  [29950/160000], accuracy = 35.57595993322204\n",
      "loss: 1.811384  [39950/160000], accuracy = 35.58698372966208\n",
      "loss: 1.636285  [49950/160000], accuracy = 35.647647647647645\n",
      "loss: 1.872505  [59950/160000], accuracy = 35.62468723936614\n",
      "loss: 1.760935  [69950/160000], accuracy = 35.62973552537527\n",
      "loss: 1.666903  [79950/160000], accuracy = 35.52470293933708\n",
      "loss: 1.731679  [89950/160000], accuracy = 35.45080600333519\n",
      "loss: 1.982234  [99950/160000], accuracy = 35.38869434717358\n",
      "loss: 1.793648  [109950/160000], accuracy = 35.3806275579809\n",
      "loss: 1.881738  [119950/160000], accuracy = 35.46477699041267\n",
      "loss: 1.748804  [129950/160000], accuracy = 35.50673335898423\n",
      "loss: 1.662754  [139950/160000], accuracy = 35.5069667738478\n",
      "loss: 1.767843  [149950/160000], accuracy = 35.51850616872291\n",
      "loss: 1.922069  [159950/160000], accuracy = 35.496092528915284\n",
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 0.035717 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.810015  [ 9950/160000], accuracy = 35.02512562814071\n",
      "loss: 1.732245  [19950/160000], accuracy = 34.94235588972431\n",
      "loss: 1.818451  [29950/160000], accuracy = 35.0550918196995\n",
      "loss: 1.713669  [39950/160000], accuracy = 35.25657071339174\n",
      "loss: 1.822592  [49950/160000], accuracy = 35.13513513513514\n",
      "loss: 1.501144  [59950/160000], accuracy = 35.17264386989157\n",
      "loss: 1.735512  [69950/160000], accuracy = 35.03216583273767\n",
      "loss: 1.867295  [79950/160000], accuracy = 35.1219512195122\n",
      "loss: 1.856919  [89950/160000], accuracy = 35.26070038910506\n",
      "loss: 1.876945  [99950/160000], accuracy = 35.30565282641321\n",
      "loss: 1.544544  [109950/160000], accuracy = 35.41518872214643\n",
      "loss: 1.796515  [119950/160000], accuracy = 35.46477699041267\n",
      "loss: 1.829213  [129950/160000], accuracy = 35.47133512889573\n",
      "loss: 1.734785  [139950/160000], accuracy = 35.50339406931047\n",
      "loss: 1.888076  [149950/160000], accuracy = 35.49583194398133\n",
      "loss: 1.695869  [159950/160000], accuracy = 35.5123476086277\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 0.035723 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.873246  [ 9950/160000], accuracy = 35.12562814070352\n",
      "loss: 1.627536  [19950/160000], accuracy = 35.30827067669173\n",
      "loss: 1.782625  [29950/160000], accuracy = 35.215358931552586\n",
      "loss: 1.777406  [39950/160000], accuracy = 35.37672090112641\n",
      "loss: 1.710043  [49950/160000], accuracy = 35.329329329329326\n",
      "loss: 1.861475  [59950/160000], accuracy = 35.31609674728941\n",
      "loss: 1.681319  [69950/160000], accuracy = 35.48963545389564\n",
      "loss: 1.767125  [79950/160000], accuracy = 35.44465290806754\n",
      "loss: 1.719299  [89950/160000], accuracy = 35.48749305169539\n",
      "loss: 1.773152  [99950/160000], accuracy = 35.509754877438716\n",
      "loss: 1.609856  [109950/160000], accuracy = 35.52796725784447\n",
      "loss: 1.661084  [119950/160000], accuracy = 35.60566902876199\n",
      "loss: 1.807867  [129950/160000], accuracy = 35.61215852250866\n",
      "loss: 1.833988  [139950/160000], accuracy = 35.566988210075024\n",
      "loss: 1.720615  [149950/160000], accuracy = 35.51517172390797\n",
      "loss: 1.949771  [159950/160000], accuracy = 35.50422006877149\n",
      "Test Error: \n",
      " Accuracy: 35.0%, Avg loss: 0.035700 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.932947  [ 9950/160000], accuracy = 34.25125628140704\n",
      "loss: 1.858637  [19950/160000], accuracy = 35.12781954887218\n",
      "loss: 1.805885  [29950/160000], accuracy = 35.51585976627713\n",
      "loss: 1.651841  [39950/160000], accuracy = 35.4468085106383\n",
      "loss: 1.668965  [49950/160000], accuracy = 35.409409409409406\n",
      "loss: 1.708797  [59950/160000], accuracy = 35.2743953294412\n",
      "loss: 1.775321  [69950/160000], accuracy = 35.336669049320946\n",
      "loss: 1.704380  [79950/160000], accuracy = 35.405878674171355\n",
      "loss: 1.915427  [89950/160000], accuracy = 35.38187882156754\n",
      "loss: 1.903568  [99950/160000], accuracy = 35.33766883441721\n",
      "loss: 1.754696  [109950/160000], accuracy = 35.36607548885857\n",
      "loss: 1.772367  [119950/160000], accuracy = 35.33722384326803\n",
      "loss: 1.735493  [129950/160000], accuracy = 35.35975375144286\n",
      "loss: 1.705109  [139950/160000], accuracy = 35.4226509467667\n",
      "loss: 1.619211  [149950/160000], accuracy = 35.39313104368123\n",
      "loss: 1.857333  [159950/160000], accuracy = 35.414817130353235\n",
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 0.035719 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.500646  [ 9950/160000], accuracy = 35.21608040201005\n",
      "loss: 1.858325  [19950/160000], accuracy = 35.34335839598997\n",
      "loss: 1.627326  [29950/160000], accuracy = 35.44908180300501\n",
      "loss: 1.669391  [39950/160000], accuracy = 35.40425531914894\n",
      "loss: 1.739191  [49950/160000], accuracy = 35.41941941941942\n",
      "loss: 1.647147  [59950/160000], accuracy = 35.54128440366973\n",
      "loss: 1.928651  [69950/160000], accuracy = 35.51822730521801\n",
      "loss: 1.787131  [79950/160000], accuracy = 35.5609756097561\n",
      "loss: 1.823905  [89950/160000], accuracy = 35.52973874374653\n",
      "loss: 1.692064  [99950/160000], accuracy = 35.582791395697846\n",
      "loss: 1.748305  [109950/160000], accuracy = 35.59254206457481\n",
      "loss: 1.718601  [119950/160000], accuracy = 35.59733222175907\n",
      "loss: 1.734362  [129950/160000], accuracy = 35.533666794921125\n",
      "loss: 1.719460  [139950/160000], accuracy = 35.471954269381925\n",
      "loss: 1.708276  [149950/160000], accuracy = 35.48382794264755\n",
      "loss: 1.804615  [159950/160000], accuracy = 35.497342919662394\n",
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 0.035698 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.988505  [ 9950/160000], accuracy = 35.447236180904525\n",
      "loss: 1.766235  [19950/160000], accuracy = 35.48370927318296\n",
      "loss: 1.770764  [29950/160000], accuracy = 35.362270450751254\n",
      "loss: 1.693795  [39950/160000], accuracy = 35.26157697121402\n",
      "loss: 1.725860  [49950/160000], accuracy = 35.431431431431434\n",
      "loss: 1.821659  [59950/160000], accuracy = 35.444537114261884\n",
      "loss: 1.586178  [69950/160000], accuracy = 35.46104360257327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.810184  [79950/160000], accuracy = 35.46966854283927\n",
      "loss: 1.898292  [89950/160000], accuracy = 35.488604780433576\n",
      "loss: 1.646261  [99950/160000], accuracy = 35.398699349674835\n",
      "loss: 1.582893  [109950/160000], accuracy = 35.4924965893588\n",
      "loss: 1.870500  [119950/160000], accuracy = 35.530637765735726\n",
      "loss: 1.544388  [129950/160000], accuracy = 35.54290111581378\n",
      "loss: 1.803315  [139950/160000], accuracy = 35.47338335119686\n",
      "loss: 1.718242  [149950/160000], accuracy = 35.52117372457486\n",
      "loss: 1.699615  [159950/160000], accuracy = 35.521100343857455\n",
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 0.035681 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.698917  [ 9950/160000], accuracy = 35.19597989949749\n",
      "loss: 1.568867  [19950/160000], accuracy = 35.62406015037594\n",
      "loss: 1.920637  [29950/160000], accuracy = 35.419031719532555\n",
      "loss: 1.760020  [39950/160000], accuracy = 35.35419274092616\n",
      "loss: 1.581004  [49950/160000], accuracy = 35.331331331331334\n",
      "loss: 1.736977  [59950/160000], accuracy = 35.382819015846536\n",
      "loss: 1.594307  [69950/160000], accuracy = 35.409578270192995\n",
      "loss: 1.863948  [79950/160000], accuracy = 35.38961851156973\n",
      "loss: 1.898747  [89950/160000], accuracy = 35.378543635352976\n",
      "loss: 1.804649  [99950/160000], accuracy = 35.398699349674835\n",
      "loss: 1.924893  [109950/160000], accuracy = 35.3706230104593\n",
      "loss: 1.825397  [119950/160000], accuracy = 35.34722801167153\n",
      "loss: 1.644629  [129950/160000], accuracy = 35.36744901885341\n",
      "loss: 1.667234  [139950/160000], accuracy = 35.3426223651304\n",
      "loss: 1.771523  [149950/160000], accuracy = 35.3784594864955\n",
      "loss: 1.564376  [159950/160000], accuracy = 35.371053454204436\n",
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 0.035685 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 1.737351  [ 9950/160000], accuracy = 35.085427135678394\n",
      "loss: 1.697963  [19950/160000], accuracy = 35.50877192982456\n",
      "loss: 1.667138  [29950/160000], accuracy = 35.228714524207014\n",
      "loss: 1.731557  [39950/160000], accuracy = 35.289111389236545\n",
      "loss: 1.706713  [49950/160000], accuracy = 35.3053053053053\n",
      "loss: 1.898458  [59950/160000], accuracy = 35.28106755629691\n",
      "loss: 1.681654  [69950/160000], accuracy = 35.399571122230164\n",
      "loss: 1.861707  [79950/160000], accuracy = 35.40462789243277\n",
      "loss: 1.560702  [89950/160000], accuracy = 35.3852140077821\n",
      "loss: 1.672846  [99950/160000], accuracy = 35.362681340670335\n",
      "loss: 1.597914  [109950/160000], accuracy = 35.3306048203729\n",
      "loss: 1.859666  [119950/160000], accuracy = 35.320550229262196\n",
      "loss: 1.930146  [129950/160000], accuracy = 35.347441323586\n",
      "loss: 1.924298  [139950/160000], accuracy = 35.316898892461595\n",
      "loss: 1.776143  [149950/160000], accuracy = 35.37445815271757\n",
      "loss: 1.967001  [159950/160000], accuracy = 35.34479524851516\n",
      "Test Error: \n",
      " Accuracy: 35.0%, Avg loss: 0.035669 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 1.731012  [ 9950/160000], accuracy = 34.8643216080402\n",
      "loss: 1.781448  [19950/160000], accuracy = 35.10275689223057\n",
      "loss: 1.785932  [29950/160000], accuracy = 35.39899833055092\n",
      "loss: 1.714858  [39950/160000], accuracy = 35.45181476846057\n",
      "loss: 1.682221  [49950/160000], accuracy = 35.52352352352352\n",
      "loss: 1.685392  [59950/160000], accuracy = 35.471226021684735\n",
      "loss: 1.825959  [69950/160000], accuracy = 35.481057898498925\n",
      "loss: 1.954422  [79950/160000], accuracy = 35.522201375859915\n",
      "loss: 2.124071  [89950/160000], accuracy = 35.55642023346304\n",
      "loss: 1.848834  [99950/160000], accuracy = 35.55377688844422\n",
      "loss: 1.753606  [109950/160000], accuracy = 35.524329240563894\n",
      "loss: 1.692454  [119950/160000], accuracy = 35.46727803251355\n",
      "loss: 1.544259  [129950/160000], accuracy = 35.41900731050404\n",
      "loss: 1.653110  [139950/160000], accuracy = 35.47338335119686\n",
      "loss: 1.789418  [149950/160000], accuracy = 35.49383127709236\n",
      "loss: 1.555642  [159950/160000], accuracy = 35.49234135667396\n",
      "Test Error: \n",
      " Accuracy: 34.8%, Avg loss: 0.035710 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 2.098701  [ 9950/160000], accuracy = 35.959798994974875\n",
      "loss: 1.768450  [19950/160000], accuracy = 35.93483709273183\n",
      "loss: 1.913805  [29950/160000], accuracy = 35.68280467445743\n",
      "loss: 1.642425  [39950/160000], accuracy = 35.60450563204005\n",
      "loss: 1.690804  [49950/160000], accuracy = 35.52352352352352\n",
      "loss: 1.768813  [59950/160000], accuracy = 35.49457881567973\n",
      "loss: 1.690426  [69950/160000], accuracy = 35.453895639742676\n",
      "loss: 1.638368  [79950/160000], accuracy = 35.470919324577864\n",
      "loss: 1.790357  [89950/160000], accuracy = 35.436353529738746\n",
      "loss: 1.723045  [99950/160000], accuracy = 35.356678339169584\n",
      "loss: 1.754254  [109950/160000], accuracy = 35.335152341973625\n",
      "loss: 1.755822  [119950/160000], accuracy = 35.34305960817007\n",
      "loss: 1.877810  [129950/160000], accuracy = 35.29973066564063\n",
      "loss: 1.738719  [139950/160000], accuracy = 35.27831368345838\n",
      "loss: 1.799286  [149950/160000], accuracy = 35.344448149383126\n",
      "loss: 1.741615  [159950/160000], accuracy = 35.36042513285402\n",
      "Test Error: \n",
      " Accuracy: 35.0%, Avg loss: 0.035680 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 1.741952  [ 9950/160000], accuracy = 35.949748743718594\n",
      "loss: 1.742510  [19950/160000], accuracy = 35.48872180451128\n",
      "loss: 1.913513  [29950/160000], accuracy = 35.235392320534224\n",
      "loss: 1.785072  [39950/160000], accuracy = 35.454317897371716\n",
      "loss: 1.838532  [49950/160000], accuracy = 35.4974974974975\n",
      "loss: 2.005360  [59950/160000], accuracy = 35.52627189324437\n",
      "loss: 1.865386  [69950/160000], accuracy = 35.46247319513939\n",
      "loss: 1.756654  [79950/160000], accuracy = 35.48843026891807\n",
      "loss: 1.799313  [89950/160000], accuracy = 35.551973318510285\n",
      "loss: 1.929744  [99950/160000], accuracy = 35.56778389194597\n",
      "loss: 1.719458  [109950/160000], accuracy = 35.54615734424738\n",
      "loss: 1.570278  [119950/160000], accuracy = 35.530637765735726\n",
      "loss: 1.711350  [129950/160000], accuracy = 35.51904578684109\n",
      "loss: 2.069908  [139950/160000], accuracy = 35.444801714898176\n",
      "loss: 1.645999  [149950/160000], accuracy = 35.48849616538846\n",
      "loss: 1.814327  [159950/160000], accuracy = 35.497968115035945\n",
      "Test Error: \n",
      " Accuracy: 34.8%, Avg loss: 0.035695 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 1.848187  [ 9950/160000], accuracy = 35.4070351758794\n",
      "loss: 1.627161  [19950/160000], accuracy = 35.59398496240601\n",
      "loss: 1.746742  [29950/160000], accuracy = 35.652754590984976\n",
      "loss: 1.733694  [39950/160000], accuracy = 35.54943679599499\n",
      "loss: 1.983370  [49950/160000], accuracy = 35.353353353353356\n",
      "loss: 1.821955  [59950/160000], accuracy = 35.41284403669725\n",
      "loss: 1.808332  [69950/160000], accuracy = 35.38384560400286\n",
      "loss: 1.804281  [79950/160000], accuracy = 35.47842401500938\n",
      "loss: 1.832392  [89950/160000], accuracy = 35.426347971095055\n",
      "loss: 1.891410  [99950/160000], accuracy = 35.480740370185096\n",
      "loss: 1.970832  [109950/160000], accuracy = 35.4206457480673\n",
      "loss: 1.590633  [119950/160000], accuracy = 35.434764485202166\n",
      "loss: 1.912991  [129950/160000], accuracy = 35.44132358599461\n",
      "loss: 1.915353  [139950/160000], accuracy = 35.407645587709894\n",
      "loss: 1.776070  [149950/160000], accuracy = 35.428476158719576\n",
      "loss: 1.613096  [159950/160000], accuracy = 35.476711472335104\n",
      "Test Error: \n",
      " Accuracy: 35.0%, Avg loss: 0.035710 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 1.803139  [ 9950/160000], accuracy = 35.517587939698494\n",
      "loss: 1.695627  [19950/160000], accuracy = 35.393483709273184\n",
      "loss: 2.063346  [29950/160000], accuracy = 35.35225375626043\n",
      "loss: 1.630877  [39950/160000], accuracy = 35.5819774718398\n",
      "loss: 1.785635  [49950/160000], accuracy = 35.43743743743744\n",
      "loss: 1.677600  [59950/160000], accuracy = 35.31776480400334\n",
      "loss: 1.739158  [69950/160000], accuracy = 35.325232308791996\n",
      "loss: 1.867080  [79950/160000], accuracy = 35.332082551594745\n",
      "loss: 1.729520  [89950/160000], accuracy = 35.3118399110617\n",
      "loss: 1.957998  [99950/160000], accuracy = 35.33566783391696\n",
      "loss: 1.686229  [109950/160000], accuracy = 35.27330604820373\n",
      "loss: 1.577131  [119950/160000], accuracy = 35.31471446436015\n",
      "loss: 1.774818  [129950/160000], accuracy = 35.32743362831859\n",
      "loss: 1.652013  [139950/160000], accuracy = 35.377634869596285\n",
      "loss: 1.724518  [149950/160000], accuracy = 35.38312770923641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.692943  [159950/160000], accuracy = 35.40418880900281\n",
      "Test Error: \n",
      " Accuracy: 34.8%, Avg loss: 0.035705 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 1.770061  [ 9950/160000], accuracy = 34.85427135678392\n",
      "loss: 1.695210  [19950/160000], accuracy = 35.12280701754386\n",
      "loss: 1.862180  [29950/160000], accuracy = 35.45909849749582\n",
      "loss: 1.696589  [39950/160000], accuracy = 35.26658322903629\n",
      "loss: 1.680952  [49950/160000], accuracy = 35.331331331331334\n",
      "loss: 1.705691  [59950/160000], accuracy = 35.436196830692246\n",
      "loss: 1.741928  [69950/160000], accuracy = 35.42816297355254\n",
      "loss: 1.673303  [79950/160000], accuracy = 35.41338336460288\n",
      "loss: 1.862987  [89950/160000], accuracy = 35.3485269594219\n",
      "loss: 1.707020  [99950/160000], accuracy = 35.38569284642321\n",
      "loss: 1.817741  [109950/160000], accuracy = 35.39699863574352\n",
      "loss: 1.826148  [119950/160000], accuracy = 35.411421425594\n",
      "loss: 1.802466  [129950/160000], accuracy = 35.39746056175452\n",
      "loss: 1.707281  [139950/160000], accuracy = 35.4269381922115\n",
      "loss: 1.609162  [149950/160000], accuracy = 35.449149716572194\n",
      "loss: 1.740278  [159950/160000], accuracy = 35.42982181931854\n",
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 0.035653 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 1.879674  [ 9950/160000], accuracy = 35.2964824120603\n",
      "loss: 1.731794  [19950/160000], accuracy = 35.233082706766915\n",
      "loss: 1.800623  [29950/160000], accuracy = 35.01502504173623\n",
      "loss: 1.864469  [39950/160000], accuracy = 35.4017521902378\n",
      "loss: 1.732839  [49950/160000], accuracy = 35.427427427427425\n",
      "loss: 1.717335  [59950/160000], accuracy = 35.397831526271894\n",
      "loss: 1.834231  [69950/160000], accuracy = 35.4024303073624\n",
      "loss: 1.723056  [79950/160000], accuracy = 35.30331457160725\n",
      "loss: 1.853188  [89950/160000], accuracy = 35.326292384658146\n",
      "loss: 1.842579  [99950/160000], accuracy = 35.317658829414704\n",
      "loss: 1.909088  [109950/160000], accuracy = 35.340609367894494\n",
      "loss: 1.640136  [119950/160000], accuracy = 35.34722801167153\n",
      "loss: 1.870943  [129950/160000], accuracy = 35.317429780684876\n",
      "loss: 1.592909  [139950/160000], accuracy = 35.327617006073595\n",
      "loss: 1.853066  [149950/160000], accuracy = 35.37645881960653\n",
      "loss: 1.659433  [159950/160000], accuracy = 35.38355736167552\n",
      "Test Error: \n",
      " Accuracy: 35.0%, Avg loss: 0.035710 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 1.807809  [ 9950/160000], accuracy = 35.38693467336683\n",
      "loss: 1.778229  [19950/160000], accuracy = 35.50877192982456\n",
      "loss: 1.659246  [29950/160000], accuracy = 35.30217028380634\n",
      "loss: 1.866350  [39950/160000], accuracy = 35.571964956195245\n",
      "loss: 1.864735  [49950/160000], accuracy = 35.47547547547548\n",
      "loss: 1.779600  [59950/160000], accuracy = 35.512927439532945\n",
      "loss: 1.661667  [69950/160000], accuracy = 35.43674052894925\n",
      "loss: 1.626035  [79950/160000], accuracy = 35.46466541588493\n",
      "loss: 1.868029  [89950/160000], accuracy = 35.49193996664814\n",
      "loss: 1.718611  [99950/160000], accuracy = 35.51175587793897\n",
      "loss: 1.976894  [109950/160000], accuracy = 35.49704411095953\n",
      "loss: 1.764947  [119950/160000], accuracy = 35.456440183409754\n",
      "loss: 1.713859  [129950/160000], accuracy = 35.43747595228934\n",
      "loss: 1.818699  [139950/160000], accuracy = 35.484816005716326\n",
      "loss: 1.842520  [149950/160000], accuracy = 35.440480160053355\n",
      "loss: 1.744902  [159950/160000], accuracy = 35.439824945295406\n",
      "Test Error: \n",
      " Accuracy: 35.3%, Avg loss: 0.035705 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 1.813024  [ 9950/160000], accuracy = 35.88944723618091\n",
      "loss: 1.603252  [19950/160000], accuracy = 35.774436090225564\n",
      "loss: 1.513085  [29950/160000], accuracy = 35.742904841402336\n",
      "loss: 1.805397  [39950/160000], accuracy = 35.546933667083856\n",
      "loss: 1.883866  [49950/160000], accuracy = 35.4034034034034\n",
      "loss: 1.909391  [59950/160000], accuracy = 35.39616346955796\n",
      "loss: 1.849362  [69950/160000], accuracy = 35.36669049320943\n",
      "loss: 1.805502  [79950/160000], accuracy = 35.318323952470294\n",
      "loss: 1.660335  [89950/160000], accuracy = 35.42412451361868\n",
      "loss: 1.668958  [99950/160000], accuracy = 35.34467233616808\n",
      "loss: 1.740452  [109950/160000], accuracy = 35.36425648021828\n",
      "loss: 1.890604  [119950/160000], accuracy = 35.45727386411004\n",
      "loss: 1.873910  [129950/160000], accuracy = 35.3943824547903\n",
      "loss: 1.752069  [139950/160000], accuracy = 35.37620578778135\n",
      "loss: 1.854481  [149950/160000], accuracy = 35.36578859619873\n",
      "loss: 1.841593  [159950/160000], accuracy = 35.39043451078462\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 0.035746 \n",
      "\n",
      "Learning Rate : 5e-06\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 1.885309  [ 9950/160000], accuracy = 35.70854271356784\n",
      "loss: 1.910087  [19950/160000], accuracy = 35.62406015037594\n",
      "loss: 1.898186  [29950/160000], accuracy = 35.43572621035059\n",
      "loss: 1.559189  [39950/160000], accuracy = 35.361702127659576\n",
      "loss: 1.800280  [49950/160000], accuracy = 35.291291291291294\n",
      "loss: 1.778616  [59950/160000], accuracy = 35.309424520433694\n",
      "loss: 1.816848  [69950/160000], accuracy = 35.28377412437455\n",
      "loss: 1.532921  [79950/160000], accuracy = 35.35709818636648\n",
      "loss: 1.717378  [89950/160000], accuracy = 35.24735964424681\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-b48454599a19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtest_dataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mtest_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Learning Rate : {lr_rate}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-86-f161d4b08ec4>\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimiser)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0moptimiser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0moptimiser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m     \u001b[0mgrad_tensors_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads)\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[0mnew_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_s = 50\n",
    "epochs = 100\n",
    "lr_rate = .000005\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(),lr = lr_rate)\n",
    "for t in range(epochs):\n",
    "    train_dataloader = DataLoader(training_data, batch_size = batch_s, shuffle = True)\n",
    "    test_dataloader = DataLoader(testing_data, batch_size = batch_s, shuffle = True)\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimiser)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "    print(f\"Learning Rate : {lr_rate}\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0636,  0.0257,  0.0445,  ...,  0.0568, -0.0705, -0.2476],\n",
      "        [-0.1350, -0.1646,  0.0975,  ...,  0.0253, -0.0586, -0.1283],\n",
      "        [-0.1585,  0.0409, -0.0578,  ..., -0.0105, -0.0284, -0.3159],\n",
      "        ...,\n",
      "        [ 0.1652,  0.0161,  0.1442,  ...,  0.0458,  0.1042, -0.0191],\n",
      "        [-0.0249,  0.0840, -0.0194,  ..., -0.2623, -0.0562,  0.0081],\n",
      "        [-0.0311,  0.1194,  0.0779,  ...,  0.0420,  0.1219,  0.1521]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0787,  0.1124, -0.1677, -0.0472,  0.0481,  0.0206,  0.0930, -0.1266,\n",
      "         0.0594, -0.0311, -0.0950,  0.0086,  0.0499, -0.1312,  0.0484,  0.0176,\n",
      "         0.0134, -0.0595, -0.0285,  0.0201, -0.0546,  0.0883, -0.0740, -0.0492,\n",
      "         0.0726,  0.1203, -0.1046,  0.0451,  0.0257, -0.0889,  0.1119, -0.1380,\n",
      "        -0.0341,  0.0121, -0.0396,  0.0386,  0.1075, -0.0140, -0.1292,  0.0869,\n",
      "         0.1160, -0.1591,  0.0618,  0.1205,  0.0570], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1.0361, 0.7025, 1.0172, 1.1386, 1.1572, 1.1116, 0.9776, 0.8058, 0.8259,\n",
      "        0.9824, 1.0421, 1.0700, 0.7741, 1.1042, 1.0500, 1.0115, 0.7422, 1.0131,\n",
      "        0.8063, 0.9111, 1.0913, 0.9180, 0.9697, 0.9172, 0.9759, 0.7510, 0.9077,\n",
      "        1.0265, 0.8651, 0.9589, 1.0425, 0.8466, 0.8204, 0.9506, 1.1057, 1.0486,\n",
      "        1.0320, 0.9384, 0.9254, 0.9700, 0.8049, 0.9679, 0.7918, 0.8809, 1.0422],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3007, -0.1164, -0.1986,  0.2988, -0.4026, -0.2920, -0.3033, -0.1650,\n",
      "        -0.0586, -0.3420, -0.2445,  0.4023, -0.0042, -0.3236, -0.3682, -0.3483,\n",
      "         0.1461, -0.1734,  0.2046, -0.0690, -0.2055, -0.0378,  0.2019, -0.2601,\n",
      "        -0.1499,  0.0917, -0.1786, -0.2602, -0.0526, -0.2613, -0.2856, -0.1001,\n",
      "        -0.0127, -0.3172, -0.3761, -0.1610, -0.4517, -0.0902, -0.3070, -0.2506,\n",
      "        -0.2394, -0.2767,  0.0800, -0.1857,  0.1938], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-7.4301e-02, -2.2044e-02, -5.9116e-02,  2.5328e-01, -6.0647e-02,\n",
      "         -1.1881e-01,  9.2162e-02, -1.4042e-01,  3.2636e-02, -1.4747e-01,\n",
      "         -4.9375e-02,  1.5732e-01,  9.5859e-02, -1.7903e-01, -1.5702e-02,\n",
      "         -6.8314e-02,  6.9936e-02,  5.2760e-02,  1.1156e-01, -5.2049e-02,\n",
      "          4.7017e-02, -1.8087e-02,  1.4792e-01,  4.9808e-02,  7.6136e-02,\n",
      "          9.1556e-02,  3.8436e-02, -1.1400e-01,  1.4821e-01, -1.1371e-01,\n",
      "          9.7697e-02, -1.3658e-01, -8.0341e-03,  6.4843e-02, -1.0452e-01,\n",
      "          5.2707e-02,  1.7908e-01,  1.4207e-01,  7.3745e-02,  1.0866e-01,\n",
      "          2.2473e-02,  2.0336e-01,  7.7160e-02, -2.8631e-02,  2.6752e-01],\n",
      "        [ 3.7440e-02, -1.7860e-02, -2.0019e-01,  1.3846e-01,  9.2353e-02,\n",
      "         -2.9278e-01,  1.4994e-01,  1.0729e-01, -5.2754e-02, -7.9453e-02,\n",
      "          8.3920e-02,  1.3554e-01,  2.6073e-02,  8.6800e-02,  2.4385e-01,\n",
      "          1.3629e-01,  6.2439e-02, -2.0563e-02,  3.9089e-02,  1.1802e-01,\n",
      "          2.2408e-01, -9.8330e-02,  1.1845e-01,  1.0751e-01,  4.1503e-02,\n",
      "          4.3536e-03,  6.8267e-02,  1.2220e-01, -1.0555e-01,  1.6139e-01,\n",
      "          8.3580e-02, -1.2886e-01,  4.4794e-02,  4.4456e-02,  2.1480e-01,\n",
      "          1.8917e-01,  2.3195e-01,  1.9327e-02,  9.6047e-02,  1.2374e-01,\n",
      "         -3.7241e-02,  2.1050e-01, -8.4803e-02,  1.2667e-01,  1.6402e-02],\n",
      "        [-6.1517e-02,  5.4658e-02, -2.0534e-01,  1.9938e-01,  1.7488e-02,\n",
      "         -2.3231e-01,  9.4759e-02,  1.1815e-01,  5.0452e-02, -2.4495e-01,\n",
      "          1.3423e-01,  1.2853e-01,  5.6144e-02,  1.8582e-01,  2.1088e-01,\n",
      "          1.8682e-02,  2.1094e-02, -8.1742e-02, -9.4667e-03,  5.9069e-02,\n",
      "          1.4222e-01, -1.4439e-01,  3.6236e-02,  9.3553e-02, -4.5362e-02,\n",
      "          1.5190e-02,  1.6851e-01,  1.6657e-01, -1.1081e-01,  2.1581e-01,\n",
      "          1.2892e-01, -4.6765e-02,  1.2946e-01, -2.5781e-02,  1.5846e-01,\n",
      "          1.7636e-01,  1.4682e-01,  6.9417e-02,  3.2206e-02,  2.1729e-02,\n",
      "          5.1806e-02,  7.7989e-02, -4.5665e-02,  5.2259e-02,  1.3196e-01],\n",
      "        [ 1.7704e-02, -4.7908e-02, -8.6252e-02,  2.5713e-01,  9.1207e-03,\n",
      "         -1.5963e-01,  1.0369e-01,  8.7421e-02,  7.8194e-02, -1.8865e-01,\n",
      "          1.8973e-01,  1.9975e-01,  1.3259e-01,  1.7464e-03,  5.1584e-02,\n",
      "          1.4101e-02,  2.3567e-01, -2.3022e-01,  7.7272e-02, -6.7746e-02,\n",
      "         -5.2822e-02, -1.8248e-01, -2.4549e-02,  1.1499e-01,  2.5068e-02,\n",
      "          1.1985e-01,  3.5748e-02, -1.5422e-01, -1.0188e-01,  1.4952e-01,\n",
      "          1.0057e-02, -1.0624e-02,  1.1475e-02,  1.4705e-02, -1.6698e-02,\n",
      "          1.1177e-01,  6.7942e-02,  3.9757e-02, -3.4429e-02,  1.1035e-01,\n",
      "          1.0111e-01, -5.2340e-02,  1.8872e-01,  9.2890e-02,  1.6164e-04],\n",
      "        [-1.3124e-01,  1.3887e-01, -2.7598e-01,  2.2063e-01, -4.2245e-02,\n",
      "         -1.0458e-01,  6.9679e-02, -4.2033e-02,  1.2214e-01, -1.7197e-01,\n",
      "          6.4483e-02,  1.3870e-01,  1.5108e-01, -9.9589e-02, -1.2710e-01,\n",
      "          2.0515e-02, -3.8637e-03, -1.2253e-01,  1.1961e-01, -1.4717e-01,\n",
      "         -1.2958e-01, -8.3483e-02,  1.8345e-01,  1.0102e-01, -2.9193e-01,\n",
      "          1.3337e-01,  5.2734e-02,  3.0880e-02, -9.6154e-02,  9.1265e-02,\n",
      "          1.0726e-01,  9.2666e-02, -9.9890e-03, -1.6870e-01, -3.7419e-03,\n",
      "          1.0173e-01, -1.8713e-02,  6.0972e-03, -4.9699e-02, -3.9250e-03,\n",
      "          3.8522e-02, -2.4324e-02,  1.2707e-01,  1.7714e-01,  9.0189e-02],\n",
      "        [ 2.2913e-01,  8.5365e-02,  1.5646e-01,  1.0788e-01,  1.2095e-02,\n",
      "          2.5067e-01, -1.9637e-01,  1.3559e-03, -9.9238e-02,  1.7815e-01,\n",
      "          8.6350e-03,  7.0582e-02,  4.0650e-02, -1.2725e-01, -2.3326e-01,\n",
      "          5.3385e-02,  4.2614e-02, -2.1481e-01, -7.6120e-02, -1.2661e-01,\n",
      "         -3.3781e-01,  9.7659e-02,  7.5304e-02, -1.6472e-01,  1.7045e-01,\n",
      "         -5.1575e-02,  1.1703e-01,  1.3511e-01,  5.7459e-02, -1.1196e-01,\n",
      "         -2.1465e-02,  8.7664e-02, -9.5132e-02,  1.9296e-01, -2.7474e-01,\n",
      "         -1.8054e-01, -2.0501e-01, -1.7307e-01,  3.5229e-02, -1.7961e-01,\n",
      "          7.2191e-02, -7.3317e-02, -1.0825e-02, -5.3820e-02,  6.9287e-02],\n",
      "        [-2.0685e-01,  4.0144e-02,  7.0976e-02,  1.0749e-01, -3.0116e-01,\n",
      "          2.1138e-02, -1.2123e-01,  3.5888e-02,  3.0188e-02, -2.5785e-01,\n",
      "         -2.3511e-01,  2.1830e-01,  5.4171e-02, -1.1667e-01, -8.3630e-02,\n",
      "         -2.9117e-01,  3.9479e-02, -4.9728e-03, -2.2823e-02, -2.9884e-02,\n",
      "         -1.0312e-02, -5.2507e-02,  1.2106e-01, -6.9098e-02, -1.3648e-02,\n",
      "          7.1348e-02, -1.0355e-01, -1.6759e-01,  6.2837e-02, -2.4966e-02,\n",
      "         -1.9418e-01,  1.0625e-01,  4.0505e-02, -2.9002e-01, -1.6643e-01,\n",
      "         -6.1439e-04, -1.9037e-01, -7.2524e-03, -1.8018e-01, -6.9702e-02,\n",
      "         -4.3563e-02, -1.0421e-01,  1.8559e-02, -1.2762e-02,  1.3196e-01],\n",
      "        [-1.1494e-01,  4.2179e-02,  7.1094e-02,  1.7321e-01, -3.5152e-01,\n",
      "          1.5483e-01, -9.2844e-02, -1.5068e-02,  1.9718e-02, -1.6255e-01,\n",
      "         -2.3132e-01,  1.0829e-01,  1.2145e-03, -2.7628e-01, -7.5481e-02,\n",
      "         -3.2588e-01,  6.4872e-03,  6.3672e-02, -3.6838e-02, -1.5256e-02,\n",
      "         -4.5049e-02,  8.1094e-02,  9.7661e-02, -8.7784e-02,  3.8438e-02,\n",
      "         -1.9142e-02, -7.6783e-02, -1.6023e-01,  3.0420e-02, -3.2832e-02,\n",
      "         -2.1151e-01,  4.8078e-02,  3.3832e-02, -1.4285e-01, -2.0002e-01,\n",
      "          2.1851e-02, -2.2034e-01,  7.7685e-04, -2.3279e-01, -1.0469e-01,\n",
      "         -1.1419e-01, -1.4905e-02,  1.0717e-02, -1.0754e-01,  6.7371e-02],\n",
      "        [-1.4373e-01,  9.3530e-02, -1.6489e-01,  2.3022e-01, -2.0199e-01,\n",
      "         -1.9943e-01,  1.0412e-01,  2.0754e-01,  5.1047e-02, -1.7254e-01,\n",
      "          1.5122e-01,  2.2829e-01,  1.3645e-01, -8.9122e-02,  7.9615e-02,\n",
      "         -5.1311e-02,  2.9754e-02, -2.6851e-02,  6.2336e-03,  4.4747e-03,\n",
      "         -3.2768e-02, -5.6472e-02,  2.9324e-01, -3.4282e-02, -1.8536e-01,\n",
      "          3.8352e-02,  7.6360e-02, -3.0131e-02,  4.1012e-02, -4.1563e-02,\n",
      "         -1.3807e-01, -3.5058e-03,  1.0592e-01, -1.8614e-01, -1.5308e-02,\n",
      "          1.1638e-01,  9.6587e-03,  6.2690e-02, -8.6807e-02,  8.2694e-02,\n",
      "          1.0752e-01, -1.4914e-02,  1.2774e-01,  2.7089e-02,  9.3062e-02]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1300, -0.1547,  0.1858,  0.0688,  0.0810,  0.0681,  0.0599, -0.0346,\n",
      "         0.0636], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.7482, 0.7284, 0.6546, 0.7626, 0.8716, 1.0301, 1.0018, 1.0945, 0.6926],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.7069, -0.0404, -0.3675, -1.3802, -1.8142,  0.9077, -0.2584,  0.9577,\n",
      "         0.3300], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for x in model.parameters():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0770, -0.0173, -0.1597,  ..., -0.0882,  0.0048, -0.0555],\n",
      "        [-0.1017, -0.0651,  0.0326,  ...,  0.1380,  0.0145,  0.0595],\n",
      "        [ 0.0231,  0.0080, -0.1274,  ..., -0.1418,  0.0490,  0.0231],\n",
      "        ...,\n",
      "        [ 0.0228,  0.0200,  0.0304,  ...,  0.0826, -0.0594, -0.0407],\n",
      "        [ 0.0520, -0.0925,  0.0698,  ..., -0.0761, -0.0641,  0.1288],\n",
      "        [-0.1009, -0.0551, -0.0063,  ...,  0.0579, -0.0042, -0.0124]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-1.0025e-01, -1.0308e-01,  1.0954e-01, -1.3122e-02,  4.3177e-02,\n",
      "         7.3575e-02,  4.3837e-02, -1.5967e-01,  1.8187e-01,  2.3326e-01,\n",
      "        -4.3246e-02, -1.1289e-01,  5.8449e-02, -1.2110e-01,  5.7925e-02,\n",
      "        -1.5396e-01, -1.3163e-01,  1.6708e-01, -1.2788e-01,  1.3268e-02,\n",
      "         1.0140e-01,  3.4252e-01,  1.5024e-01, -3.9175e-02, -1.3434e-01,\n",
      "        -3.1853e-01,  1.5481e-01,  2.7301e-01,  2.3915e-01, -4.4431e-02,\n",
      "        -4.2798e-02,  1.2929e-01,  3.6477e-02, -2.2467e-02,  2.1653e-01,\n",
      "         9.9343e-04, -1.6598e-01,  1.5455e-01, -7.9143e-03, -8.8547e-02,\n",
      "        -3.9119e-01, -6.1415e-02, -1.5245e-01,  2.0366e-01,  6.1055e-02,\n",
      "         9.0184e-02, -3.2418e-04,  5.1891e-02, -1.3615e-01,  3.8363e-02,\n",
      "         7.7584e-02,  1.1657e-01,  3.7235e-02,  1.2199e-01, -1.9959e-02,\n",
      "         1.5604e-01,  1.2882e-01,  6.0212e-02, -1.0630e-01, -2.6352e-02,\n",
      "         1.0021e-01,  5.1747e-02, -6.8097e-02, -4.7217e-02, -3.0259e-02,\n",
      "         1.2938e-01,  1.8793e-01,  2.0446e-01, -2.2836e-02,  2.4853e-01,\n",
      "         1.3074e-01,  2.2922e-01, -4.3378e-02,  4.7014e-02,  6.4249e-02,\n",
      "         2.7369e-02,  1.6669e-01,  1.7362e-01,  1.2676e-01, -5.3520e-02,\n",
      "        -2.2902e-02, -3.3468e-02,  1.2984e-01,  2.8929e-02, -4.2687e-02,\n",
      "        -1.2611e-01,  6.6725e-02, -2.3474e-01, -1.0472e-01, -2.9258e-02,\n",
      "         1.2316e-02,  6.8658e-02,  1.5592e-01, -1.0142e-01,  5.7369e-02,\n",
      "         2.3429e-01, -3.2695e-02,  2.5662e-03,  1.1743e-02,  1.5240e-01],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1040,  0.0447, -0.1100,  ..., -0.0238,  0.0185,  0.1288],\n",
      "        [-0.0474,  0.0160, -0.0878,  ..., -0.1009, -0.0030, -0.0946],\n",
      "        [-0.0569,  0.0905,  0.0804,  ...,  0.0095, -0.0642,  0.0542],\n",
      "        ...,\n",
      "        [ 0.0217, -0.0057, -0.0911,  ..., -0.0234,  0.0553, -0.1128],\n",
      "        [-0.0430,  0.0893,  0.0239,  ..., -0.0434,  0.1080, -0.0290],\n",
      "        [-0.0239, -0.0343,  0.0274,  ..., -0.0577, -0.0395,  0.0686]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1453,  0.1372,  0.1079,  0.4820,  0.0819,  0.0586,  0.1766, -0.0227,\n",
      "        -0.3790,  0.1109, -0.2406,  0.1988,  0.5169,  0.1758,  0.1420, -0.0262,\n",
      "         0.2483,  0.2793,  0.3733, -0.0139,  0.4480, -0.1335,  0.4099,  0.0622,\n",
      "         0.5365, -0.0279,  0.2816,  0.0651,  0.3649,  0.4661,  0.0609,  0.1791,\n",
      "         0.0966,  0.1412,  0.6045,  0.5210,  0.4237,  0.0287, -0.0478,  0.3157],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-6.2687e-02,  2.8642e-02, -5.7464e-02, -4.4517e-02, -1.1649e-01,\n",
      "         -5.1793e-02,  6.9824e-02, -4.0824e-02,  7.9017e-02, -3.4782e-02,\n",
      "          9.7398e-02, -1.0877e-01, -5.2256e-02,  8.9858e-02,  1.7521e-02,\n",
      "          1.1340e-01,  1.3604e-01, -1.0738e-01, -7.0135e-02,  8.0124e-02,\n",
      "          5.1314e-04,  2.1905e-02,  1.0230e-01,  4.0967e-02, -2.0986e-02,\n",
      "          7.6089e-02,  2.3897e-02, -1.5625e-02, -8.2229e-02,  1.0316e-01,\n",
      "          7.0571e-02, -5.7579e-02, -1.9604e-02, -1.0656e-01, -1.9799e-01,\n",
      "          2.6807e-01,  1.5725e-01,  7.0354e-02, -9.4106e-02,  9.5495e-02],\n",
      "        [-1.2688e-01, -1.7806e-01, -4.5642e-02,  3.6376e-01, -6.9335e-02,\n",
      "         -5.3149e-03, -3.1777e-02, -2.9460e-02,  9.3988e-02,  1.8743e-01,\n",
      "          1.2128e-01,  2.3412e-01, -7.1551e-02,  7.8298e-02, -1.0623e-01,\n",
      "          9.7675e-02,  2.3908e-01,  9.4676e-02,  1.2868e-01,  9.3958e-02,\n",
      "         -2.8236e-01,  1.0367e-01,  2.6569e-01, -1.8028e-01, -2.7540e-01,\n",
      "         -1.0599e-02,  1.5155e-01,  2.8374e-02,  1.9591e-01, -5.6504e-02,\n",
      "         -1.5228e-02,  3.3329e-03, -1.7547e-01, -1.8206e-01,  5.3489e-02,\n",
      "          3.8461e-01,  2.8814e-01, -4.6127e-02,  1.3327e-01,  2.1222e-01],\n",
      "        [ 2.0943e-02, -4.4756e-02,  1.6859e-02,  1.0971e-01, -3.1056e-02,\n",
      "         -1.1373e-02, -1.1181e-01,  1.4820e-01, -1.0658e-02,  5.2050e-02,\n",
      "          6.5209e-02,  1.8383e-01,  2.4806e-02,  2.8538e-02, -8.4054e-02,\n",
      "         -9.5315e-02,  1.6546e-02,  9.7978e-02,  1.2859e-01,  1.1419e-01,\n",
      "         -2.5847e-01,  1.7897e-02,  2.8525e-01, -2.1490e-02, -3.0847e-01,\n",
      "          6.2564e-02,  9.1463e-02, -3.7948e-02,  1.3814e-01,  6.4776e-02,\n",
      "          4.6239e-02, -4.6159e-02, -1.1158e-01, -1.4348e-01, -2.1823e-02,\n",
      "          9.8224e-02,  2.5177e-01, -4.5423e-02,  1.4178e-01,  1.6358e-01],\n",
      "        [ 5.2358e-02,  4.7808e-02,  3.9065e-02,  1.3251e-01,  5.4200e-02,\n",
      "         -1.2157e-01, -9.9844e-02, -2.1210e-02, -3.6565e-02,  1.2427e-01,\n",
      "          3.4652e-02,  1.3693e-01, -1.6924e-01, -4.7428e-02, -4.3354e-02,\n",
      "         -2.2586e-02, -1.2421e-01, -1.5578e-01, -4.4283e-02, -1.0825e-01,\n",
      "         -1.8439e-01,  1.7722e-01,  1.5517e-01,  6.1615e-02, -1.6489e-01,\n",
      "          8.7927e-02, -9.8846e-03,  1.1429e-01, -1.6338e-01, -1.9047e-01,\n",
      "          1.4583e-01, -1.4557e-01,  1.3261e-02, -3.5597e-02, -2.9727e-01,\n",
      "         -3.8768e-02,  2.8618e-02,  8.6964e-02,  1.6038e-01, -9.3503e-02],\n",
      "        [ 2.6949e-02, -1.5390e-01, -6.6504e-02, -6.1103e-02, -8.6686e-02,\n",
      "         -2.6820e-02, -2.4344e-02,  9.5838e-02,  1.3803e-01,  1.2098e-01,\n",
      "          6.8331e-02,  6.3076e-02, -1.7017e-01, -2.4746e-02, -1.0336e-01,\n",
      "         -1.4854e-01, -6.0806e-03, -1.6808e-01, -1.1389e-01,  8.0250e-02,\n",
      "         -1.0894e-01,  1.7480e-01,  6.8128e-03, -2.0247e-03, -1.7906e-01,\n",
      "          5.4267e-02, -4.2126e-02, -1.4139e-02, -1.1175e-01, -2.8500e-01,\n",
      "          2.4092e-02, -7.1666e-02,  6.4685e-02,  9.6716e-02, -3.5705e-01,\n",
      "         -1.1409e-01, -3.7949e-02,  7.2163e-02,  8.6282e-02, -1.1872e-01],\n",
      "        [ 3.4890e-01,  3.7715e-02,  5.1554e-02,  1.4893e-01,  7.8048e-03,\n",
      "          7.0464e-02,  1.8501e-02,  9.5910e-02, -1.7361e-01,  1.0625e-01,\n",
      "         -1.7491e-01, -1.5457e-01,  7.2783e-01,  1.9328e-01,  4.1956e-03,\n",
      "         -2.5018e-02, -1.3662e-01, -3.2139e-01, -2.7069e-01, -2.8153e-02,\n",
      "          2.5072e-01,  9.3197e-02, -2.4309e-01,  1.0829e-01,  4.6531e-01,\n",
      "         -6.2737e-02,  2.7136e-01, -1.4085e-01,  2.3506e-01,  1.1477e-01,\n",
      "          2.3416e-02, -1.7157e-01,  1.6484e-01,  1.0702e-01,  2.1041e-01,\n",
      "         -3.0558e-01, -4.0008e-01, -1.3905e-01, -1.4549e-01,  1.6516e-01],\n",
      "        [ 5.4543e-02, -5.3417e-02,  6.4426e-02, -2.5466e-01, -1.2844e-01,\n",
      "         -9.2942e-02, -9.2583e-02,  3.0712e-02, -2.5819e-02,  8.8430e-02,\n",
      "         -6.6358e-02, -1.6996e-01, -2.6806e-01,  1.3058e-01,  1.1502e-01,\n",
      "         -1.7141e-01, -1.3494e-01, -1.0187e-01,  1.1300e-01,  6.3385e-03,\n",
      "          9.0517e-02,  1.2360e-02, -1.2617e-01, -6.7232e-03,  7.7705e-02,\n",
      "          1.1888e-01, -1.1121e-01, -9.8906e-02, -7.3681e-02,  1.2272e-01,\n",
      "         -1.0765e-01,  1.3422e-01,  1.2264e-01,  1.2373e-01, -1.4453e-02,\n",
      "         -5.7136e-02, -1.0119e-02, -9.8956e-02,  2.9442e-02, -2.9693e-03],\n",
      "        [ 8.1871e-02, -2.9041e-02,  1.1743e-01, -4.0244e-01,  1.0259e-01,\n",
      "         -1.4842e-02,  1.6517e-01, -1.6640e-02, -1.7689e-01,  8.9267e-02,\n",
      "         -8.3098e-02, -3.5549e-03, -2.7882e-01,  1.3863e-01,  1.1074e-01,\n",
      "         -8.7133e-02, -1.7626e-01, -2.6347e-02,  1.8504e-01,  1.5823e-02,\n",
      "          2.0517e-01, -1.5465e-01, -2.5077e-01,  1.0461e-01,  2.2576e-01,\n",
      "          7.1258e-02, -1.3291e-01,  1.1863e-01, -1.2447e-01,  2.0091e-01,\n",
      "          1.8031e-01, -1.1195e-02,  1.0350e-01,  7.3351e-02,  1.9577e-01,\n",
      "         -4.1271e-01, -1.2208e-01,  3.4881e-02, -6.5844e-02, -1.6163e-01],\n",
      "        [-8.9126e-02, -1.2020e-01,  7.6240e-03, -4.5261e-02, -9.6670e-02,\n",
      "          1.0262e-01,  9.2946e-02, -7.8502e-02, -3.7012e-02, -1.1879e-01,\n",
      "         -3.0356e-02,  2.7370e-02, -1.6670e-01,  7.5777e-02,  8.8713e-02,\n",
      "         -3.5006e-02,  8.6642e-02, -6.0171e-02,  1.3605e-01, -3.3774e-02,\n",
      "          1.9644e-02, -4.1081e-02,  1.4166e-01, -2.8055e-02, -2.8653e-02,\n",
      "          3.6953e-02,  6.7314e-02,  1.5383e-01,  2.3915e-02,  1.2655e-01,\n",
      "          6.8508e-03,  1.4613e-01,  2.0738e-02, -8.3928e-02,  5.7546e-02,\n",
      "          9.4586e-02,  1.5847e-02, -9.2021e-02,  5.7707e-02,  1.5562e-01]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.5230,  0.4506,  0.0046, -0.6626, -0.8967,  0.6719, -0.3558,  0.7582,\n",
      "         0.2415], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for x in model.parameters():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(next(iter(train_dataloader))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = nn.functional.softmax(model(torch.tensor(datt1.values.astype(np.float32))),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100000, 9])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(y.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class_1</th>\n",
       "      <th>Class_2</th>\n",
       "      <th>Class_3</th>\n",
       "      <th>Class_4</th>\n",
       "      <th>Class_5</th>\n",
       "      <th>Class_6</th>\n",
       "      <th>Class_7</th>\n",
       "      <th>Class_8</th>\n",
       "      <th>Class_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.054657</td>\n",
       "      <td>0.307126</td>\n",
       "      <td>0.140176</td>\n",
       "      <td>0.021094</td>\n",
       "      <td>0.021094</td>\n",
       "      <td>0.213183</td>\n",
       "      <td>0.032939</td>\n",
       "      <td>0.074276</td>\n",
       "      <td>0.135455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.041360</td>\n",
       "      <td>0.126510</td>\n",
       "      <td>0.084928</td>\n",
       "      <td>0.025120</td>\n",
       "      <td>0.025120</td>\n",
       "      <td>0.219802</td>\n",
       "      <td>0.074822</td>\n",
       "      <td>0.258397</td>\n",
       "      <td>0.143942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.032708</td>\n",
       "      <td>0.040827</td>\n",
       "      <td>0.034627</td>\n",
       "      <td>0.006995</td>\n",
       "      <td>0.006995</td>\n",
       "      <td>0.517231</td>\n",
       "      <td>0.053770</td>\n",
       "      <td>0.221040</td>\n",
       "      <td>0.085806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.042343</td>\n",
       "      <td>0.091037</td>\n",
       "      <td>0.065261</td>\n",
       "      <td>0.016633</td>\n",
       "      <td>0.016633</td>\n",
       "      <td>0.322088</td>\n",
       "      <td>0.070627</td>\n",
       "      <td>0.240461</td>\n",
       "      <td>0.134917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.050013</td>\n",
       "      <td>0.130166</td>\n",
       "      <td>0.086135</td>\n",
       "      <td>0.023270</td>\n",
       "      <td>0.023270</td>\n",
       "      <td>0.262936</td>\n",
       "      <td>0.068375</td>\n",
       "      <td>0.209804</td>\n",
       "      <td>0.146031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Class_1   Class_2   Class_3   Class_4   Class_5   Class_6   Class_7  \\\n",
       "0  0.054657  0.307126  0.140176  0.021094  0.021094  0.213183  0.032939   \n",
       "1  0.041360  0.126510  0.084928  0.025120  0.025120  0.219802  0.074822   \n",
       "2  0.032708  0.040827  0.034627  0.006995  0.006995  0.517231  0.053770   \n",
       "3  0.042343  0.091037  0.065261  0.016633  0.016633  0.322088  0.070627   \n",
       "4  0.050013  0.130166  0.086135  0.023270  0.023270  0.262936  0.068375   \n",
       "\n",
       "    Class_8   Class_9  \n",
       "0  0.074276  0.135455  \n",
       "1  0.258397  0.143942  \n",
       "2  0.221040  0.085806  \n",
       "3  0.240461  0.134917  \n",
       "4  0.209804  0.146031  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis = [f\"Class_{i}\" for i in range(1,10)]\n",
    "sub.columns = lis\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Class_1</th>\n",
       "      <th>Class_2</th>\n",
       "      <th>Class_3</th>\n",
       "      <th>Class_4</th>\n",
       "      <th>Class_5</th>\n",
       "      <th>Class_6</th>\n",
       "      <th>Class_7</th>\n",
       "      <th>Class_8</th>\n",
       "      <th>Class_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200000</td>\n",
       "      <td>0.054657</td>\n",
       "      <td>0.307126</td>\n",
       "      <td>0.140176</td>\n",
       "      <td>0.021094</td>\n",
       "      <td>0.021094</td>\n",
       "      <td>0.213183</td>\n",
       "      <td>0.032939</td>\n",
       "      <td>0.074276</td>\n",
       "      <td>0.135455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200001</td>\n",
       "      <td>0.041360</td>\n",
       "      <td>0.126510</td>\n",
       "      <td>0.084928</td>\n",
       "      <td>0.025120</td>\n",
       "      <td>0.025120</td>\n",
       "      <td>0.219802</td>\n",
       "      <td>0.074822</td>\n",
       "      <td>0.258397</td>\n",
       "      <td>0.143942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200002</td>\n",
       "      <td>0.032708</td>\n",
       "      <td>0.040827</td>\n",
       "      <td>0.034627</td>\n",
       "      <td>0.006995</td>\n",
       "      <td>0.006995</td>\n",
       "      <td>0.517231</td>\n",
       "      <td>0.053770</td>\n",
       "      <td>0.221040</td>\n",
       "      <td>0.085806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200003</td>\n",
       "      <td>0.042343</td>\n",
       "      <td>0.091037</td>\n",
       "      <td>0.065261</td>\n",
       "      <td>0.016633</td>\n",
       "      <td>0.016633</td>\n",
       "      <td>0.322088</td>\n",
       "      <td>0.070627</td>\n",
       "      <td>0.240461</td>\n",
       "      <td>0.134917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200004</td>\n",
       "      <td>0.050013</td>\n",
       "      <td>0.130166</td>\n",
       "      <td>0.086135</td>\n",
       "      <td>0.023270</td>\n",
       "      <td>0.023270</td>\n",
       "      <td>0.262936</td>\n",
       "      <td>0.068375</td>\n",
       "      <td>0.209804</td>\n",
       "      <td>0.146031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id   Class_1   Class_2   Class_3   Class_4   Class_5   Class_6  \\\n",
       "0  200000  0.054657  0.307126  0.140176  0.021094  0.021094  0.213183   \n",
       "1  200001  0.041360  0.126510  0.084928  0.025120  0.025120  0.219802   \n",
       "2  200002  0.032708  0.040827  0.034627  0.006995  0.006995  0.517231   \n",
       "3  200003  0.042343  0.091037  0.065261  0.016633  0.016633  0.322088   \n",
       "4  200004  0.050013  0.130166  0.086135  0.023270  0.023270  0.262936   \n",
       "\n",
       "    Class_7   Class_8   Class_9  \n",
       "0  0.032939  0.074276  0.135455  \n",
       "1  0.074822  0.258397  0.143942  \n",
       "2  0.053770  0.221040  0.085806  \n",
       "3  0.070627  0.240461  0.134917  \n",
       "4  0.068375  0.209804  0.146031  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub1 = pd.concat([iden,sub],axis = 1)\n",
    "sub1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Class_1</th>\n",
       "      <th>Class_2</th>\n",
       "      <th>Class_3</th>\n",
       "      <th>Class_4</th>\n",
       "      <th>Class_5</th>\n",
       "      <th>Class_6</th>\n",
       "      <th>Class_7</th>\n",
       "      <th>Class_8</th>\n",
       "      <th>Class_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200000</td>\n",
       "      <td>0.0547</td>\n",
       "      <td>0.3071</td>\n",
       "      <td>0.1402</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.2132</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.1355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200001</td>\n",
       "      <td>0.0414</td>\n",
       "      <td>0.1265</td>\n",
       "      <td>0.0849</td>\n",
       "      <td>0.0251</td>\n",
       "      <td>0.0251</td>\n",
       "      <td>0.2198</td>\n",
       "      <td>0.0748</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.1439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200002</td>\n",
       "      <td>0.0327</td>\n",
       "      <td>0.0408</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.5172</td>\n",
       "      <td>0.0538</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>0.0858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200003</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>0.0910</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.3221</td>\n",
       "      <td>0.0706</td>\n",
       "      <td>0.2405</td>\n",
       "      <td>0.1349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200004</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.1302</td>\n",
       "      <td>0.0861</td>\n",
       "      <td>0.0233</td>\n",
       "      <td>0.0233</td>\n",
       "      <td>0.2629</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.1460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  Class_1  Class_2  Class_3  Class_4  Class_5  Class_6  Class_7  \\\n",
       "0  200000   0.0547   0.3071   0.1402   0.0211   0.0211   0.2132   0.0329   \n",
       "1  200001   0.0414   0.1265   0.0849   0.0251   0.0251   0.2198   0.0748   \n",
       "2  200002   0.0327   0.0408   0.0346   0.0070   0.0070   0.5172   0.0538   \n",
       "3  200003   0.0423   0.0910   0.0653   0.0166   0.0166   0.3221   0.0706   \n",
       "4  200004   0.0500   0.1302   0.0861   0.0233   0.0233   0.2629   0.0684   \n",
       "\n",
       "   Class_8  Class_9  \n",
       "0   0.0743   0.1355  \n",
       "1   0.2584   0.1439  \n",
       "2   0.2210   0.0858  \n",
       "3   0.2405   0.1349  \n",
       "4   0.2098   0.1460  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(1,10):\n",
    "    sub1[f\"Class_{i}\"] = sub1[f\"Class_{i}\"].round(decimals = 4)\n",
    "sub1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1.to_csv('submission4.csv',index = False,header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
